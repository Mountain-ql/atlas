diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index 7a35a6e..9916ed2 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -356,3 +356,6 @@
 347	i386	process_vm_readv	sys_process_vm_readv		compat_sys_process_vm_readv
 348	i386	process_vm_writev	sys_process_vm_writev		compat_sys_process_vm_writev
 349	i386	kcmp			sys_kcmp
+350	i386	atlas_next		sys_atlas_next
+351	i386	atlas_submit		sys_atlas_submit
+352	i386	atlas_debug		sys_atlas_debug
diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index a582bfe..43f9836 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -319,6 +319,9 @@
 310	64	process_vm_readv	sys_process_vm_readv
 311	64	process_vm_writev	sys_process_vm_writev
 312	common	kcmp			sys_kcmp
+313	common	atlas_next		sys_atlas_next
+314	64	atlas_submit		sys_atlas_submit
+315	common	atlas_debug		sys_atlas_debug
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/debian.quantal/config/amd64/config.flavour.atlas b/debian.quantal/config/amd64/config.flavour.atlas
new file mode 100644
index 0000000..06d8b53
--- /dev/null
+++ b/debian.quantal/config/amd64/config.flavour.atlas
@@ -0,0 +1,3 @@
+#
+# Config options for config.flavour.atlas automatically generated by splitconfig.pl
+#
diff --git a/debian.quantal/config/i386/config.flavour.atlas b/debian.quantal/config/i386/config.flavour.atlas
new file mode 100644
index 0000000..06d8b53
--- /dev/null
+++ b/debian.quantal/config/i386/config.flavour.atlas
@@ -0,0 +1,3 @@
+#
+# Config options for config.flavour.atlas automatically generated by splitconfig.pl
+#
diff --git a/debian.quantal/control.d/vars.atlas b/debian.quantal/control.d/vars.atlas
new file mode 100644
index 0000000..83e429e
--- /dev/null
+++ b/debian.quantal/control.d/vars.atlas
@@ -0,0 +1,6 @@
+arch="i386 amd64"
+supported="ATLAS"
+target="Experimental ATLAS scheduler."
+desc="=HUMAN= SMP"
+bootloader="grub-pc | grub-efi-amd64 | grub-efi-ia32 | grub | lilo (>= 19.1)"
+provides="kvm-api-4, redhat-cluster-modules, ivtv-modules, ndiswrapper-modules-1.9"
diff --git a/debian.quantal/rules.d/amd64.mk b/debian.quantal/rules.d/amd64.mk
index 0ef1186..8a98fc7 100644
--- a/debian.quantal/rules.d/amd64.mk
+++ b/debian.quantal/rules.d/amd64.mk
@@ -2,7 +2,7 @@ human_arch	= 64 bit x86
 build_arch	= x86_64
 header_arch	= $(build_arch)
 defconfig	= defconfig
-flavours	= generic
+flavours	= atlas
 build_image	= bzImage
 kernel_file	= arch/$(build_arch)/boot/bzImage
 install_file	= vmlinuz
diff --git a/debian.quantal/rules.d/i386.mk b/debian.quantal/rules.d/i386.mk
index 3e82c65..35feef3 100644
--- a/debian.quantal/rules.d/i386.mk
+++ b/debian.quantal/rules.d/i386.mk
@@ -2,7 +2,7 @@ human_arch	= 32 bit x86
 build_arch	= i386
 header_arch	= x86_64
 defconfig	= defconfig
-flavours        = generic
+flavours        = atlas
 build_image	= bzImage
 kernel_file	= arch/$(build_arch)/boot/bzImage
 install_file	= vmlinuz
diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9e65eff..cb2f5ca 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -159,6 +159,13 @@ extern struct cred init_cred;
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
 		.time_slice	= RR_TIMESLICE,				\
 	},								\
+	.atlas  = {						\
+		.state = ATLAS_UNDEF,		\
+		.flags = 0,					\
+		.jobs = LIST_HEAD_INIT(tsk.atlas.jobs),		\
+		.job  = NULL,		\
+		.jobs_lock = __SPIN_LOCK_UNLOCKED(tsk.atlas.jobs_lock),	\
+	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	INIT_PUSHABLE_TASKS(tsk)					\
 	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a1f493..596124d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,8 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+/* SCHED_ATLAS:  ATLAS Scheduler*/
+#define SCHED_ATLAS		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1211,6 +1213,28 @@ struct sched_rt_entity {
 #endif
 };
 
+enum atlas_state {
+		ATLAS_UNDEF,
+		ATLAS_BLOCKED,
+		ATLAS_RUNNING,
+};
+
+
+struct sched_atlas_entity {
+	struct rb_node     run_node; /*for normal operation*/
+	//struct list_head   run_list;  ??
+	struct list_head   list;     /*for initialization*/
+	unsigned int       state;
+	unsigned long      flags;
+	unsigned int       on_rq;
+	//struct atlas_rq    *atlas_rq; //needed?
+	
+	struct atlas_job  *job;
+	struct list_head         jobs;
+	spinlock_t               jobs_lock; // to lock list and recent submission
+	struct hrtimer 			 timer;
+};
+
 /*
  * default timeslice is 100 msecs (used only for SCHED_RR tasks).
  * Timeslices get refilled after they expire.
@@ -1244,6 +1268,7 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+	struct sched_atlas_entity atlas;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -2002,6 +2027,10 @@ extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 
+extern unsigned int sysctl_sched_atlas_min_slack;
+extern unsigned int sysctl_sched_atlas_slice;
+
+
 enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_NONE,
 	SCHED_TUNABLESCALING_LOG,
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 19439c7..9c26268 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -857,6 +857,14 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 				      const struct iovec __user *rvec,
 				      unsigned long riovcnt,
 				      unsigned long flags);
+asmlinkage long sys_atlas_next(void);
+asmlinkage long sys_atlas_submit(pid_t pid,
+					struct timeval __user *exectime,
+					struct timeval __user *deadline,
+					int time_base);
+asmlinkage long sys_atlas_debug(int operation,
+					int arg1,
+					int arg2);
 
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index ea7a203..b10346c 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -150,6 +150,84 @@ TRACE_EVENT(sched_switch,
 );
 
 /*
+ * Tracepoint for pick_next_task.
+ */
+TRACE_EVENT(sched_pick_next_task,
+
+	TP_PROTO(struct task_struct *p),
+
+	TP_ARGS(p),
+
+	TP_STRUCT__entry(
+		__array(	char,	p_comm,	TASK_COMM_LEN	)
+		__field(	pid_t,	pid			)
+		__field(	int,	policy		)
+		__field(	unsigned long,	flags	)
+		__field(	int,	has_sub			)
+		__field(    void *, job	            )
+		__field(	s64,	sdeadline		)
+		__field(	s64,	deadline		)
+		__field(	s64,	sexectime		)
+		__field(	s64,	exectime		)
+		__field(	s64,	now				)
+	),
+
+	TP_fast_assign(
+		memcpy(__entry->p_comm, p->comm, TASK_COMM_LEN);
+		__entry->pid	 = p->pid;
+		__entry->policy  = p->policy;
+		__entry->flags   = p->atlas.flags;
+		__entry->job     = p->atlas.job;
+		__entry->sdeadline = __entry->job ? ktime_to_ns(p->atlas.job->sdeadline) : 0;
+		__entry->deadline  = __entry->job ? ktime_to_ns(p->atlas.job->deadline) : 0;
+		__entry->sexectime = __entry->job ? ktime_to_ns(p->atlas.job->sexectime) : 0;
+		__entry->exectime  = __entry->job ? ktime_to_ns(p->atlas.job->exectime) : 0;
+		__entry->now       = ktime_to_ns(ktime_get());
+	),
+
+	TP_printk("pid=%d",
+		__entry->pid)
+);
+
+/*
+ * Tracepoint for queuing:
+ */
+DECLARE_EVENT_CLASS(sched_queue_template,
+
+	TP_PROTO(struct task_struct *p, int rq_cpu),
+
+	TP_ARGS(p, rq_cpu),
+
+	TP_STRUCT__entry(
+		__array(	char,	comm,	TASK_COMM_LEN	)
+		__field(	pid_t,	pid			)
+		__field(	int,	policy		)
+		__field(	int,	rq_cpu		)
+		__field(	s64,	now			)
+	),
+
+	TP_fast_assign(
+		memcpy(__entry->comm, p->comm, TASK_COMM_LEN);
+		__entry->pid		= p->pid;
+		__entry->policy     = p->policy;
+		__entry->rq_cpu     = rq_cpu;
+		__entry->now        = ktime_to_ns(ktime_get());
+	),
+
+	TP_printk("pid=%d",
+		__entry->pid)
+);
+
+DEFINE_EVENT(sched_queue_template, sched_enqueue_task,
+	     TP_PROTO(struct task_struct *p, int cpu_rq),
+	     TP_ARGS(p, cpu_rq));
+
+DEFINE_EVENT(sched_queue_template, sched_dequeue_task,
+	     TP_PROTO(struct task_struct *p, int cpu_rq),
+	     TP_ARGS(p, cpu_rq));
+
+
+/*
  * Tracepoint for a task being migrated:
  */
 TRACE_EVENT(sched_migrate_task,
diff --git a/kernel/exit.c b/kernel/exit.c
index 46ce8da..f438a6a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -60,6 +60,7 @@
 #include <asm/mmu_context.h>
 
 static void exit_mm(struct task_struct * tsk);
+extern void exit_atlas(struct task_struct * tsk);
 
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
@@ -991,6 +992,7 @@ void do_exit(long code)
 		acct_process();
 	trace_sched_process_exit(tsk);
 
+	exit_atlas(tsk);
 	exit_sem(tsk);
 	exit_shm(tsk);
 	exit_files(tsk);
diff --git a/kernel/printk.c b/kernel/printk.c
index 146827f..b827392 100644
--- a/kernel/printk.c
+++ b/kernel/printk.c
@@ -1875,21 +1875,33 @@ int is_console_locked(void)
 /*
  * Delayed printk version, for scheduler-internal messages:
  */
-#define PRINTK_BUF_SIZE		512
+#define PRINTK_BUF_SIZE		128
+#define PRINTK_BUF_NR		256
 
 #define PRINTK_PENDING_WAKEUP	0x01
 #define PRINTK_PENDING_SCHED	0x02
 
 static DEFINE_PER_CPU(int, printk_pending);
-static DEFINE_PER_CPU(char [PRINTK_BUF_SIZE], printk_sched_buf);
+
+typedef struct {
+	int idx;
+	char buf[PRINTK_BUF_NR][PRINTK_BUF_SIZE];
+} printk_sched_buf_t;
+
+static DEFINE_PER_CPU(printk_sched_buf_t, printk_sched_buf) = {.idx = 0 };
 
 void printk_tick(void)
 {
 	if (__this_cpu_read(printk_pending)) {
 		int pending = __this_cpu_xchg(printk_pending, 0);
 		if (pending & PRINTK_PENDING_SCHED) {
-			char *buf = __get_cpu_var(printk_sched_buf);
-			printk(KERN_WARNING "[sched_delayed] %s", buf);
+			printk_sched_buf_t *data = &__get_cpu_var(printk_sched_buf);
+			int i;
+			for (i = 0; i < data->idx; ++i) {
+				char *buf = data->buf[i]; 
+				printk(KERN_WARNING "[sched_delayed] %s", buf);
+			}
+			data->idx = 0;
 		}
 		if (pending & PRINTK_PENDING_WAKEUP)
 			wake_up_interruptible(&log_wait);
@@ -2362,16 +2374,19 @@ int printk_sched(const char *fmt, ...)
 {
 	unsigned long flags;
 	va_list args;
+	printk_sched_buf_t *data;
 	char *buf;
 	int r;
 
 	local_irq_save(flags);
-	buf = __get_cpu_var(printk_sched_buf);
-
+	data = &__get_cpu_var(printk_sched_buf);
+	BUG_ON(data->idx >= PRINTK_BUF_NR);
+	buf = data->buf[data->idx++];
+	
 	va_start(args, fmt);
 	r = vsnprintf(buf, PRINTK_BUF_SIZE, fmt, args);
 	va_end(args);
-
+	
 	__this_cpu_or(printk_pending, PRINTK_PENDING_SCHED);
 	local_irq_restore(flags);
 
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 173ea52..fc2ebc7 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o
+obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o atlas.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/atlas.c b/kernel/sched/atlas.c
new file mode 100644
index 0000000..1c7f37f
--- /dev/null
+++ b/kernel/sched/atlas.c
@@ -0,0 +1,1894 @@
+#include <linux/syscalls.h>
+#include <linux/rbtree.h>
+#include <linux/slab.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include "sched.h"
+
+unsigned int sysctl_sched_atlas_min_slack = 1000000ULL;
+unsigned int sysctl_sched_atlas_slice    = 10000000ULL;
+
+const struct sched_class atlas_sched_class;
+
+extern u64 cfs_sched_period(unsigned long nr_running);
+
+
+#define SLACKTIME_CFS    0x1
+#define SLACKTIME_ATLAS  0x2
+#define SLACKTIME        0x3
+
+//#define ATLAS_DEBUG
+
+enum {
+		DEBUG_SYS_NEXT       = 1UL << 0,
+		DEBUG_SYS_SUBMIT     = 1UL << 1,
+		DEBUG_ENQUEUE        = 1UL << 2,
+		DEBUG_DEQUEUE        = 1UL << 3,
+		DEBUG_PICK_NEXT_TASK = 1UL << 4,
+		DEBUG_SET_CURR_TASK  = 1UL << 5,
+		DEBUG_SWITCHED_TO    = 1UL << 6,
+		DEBUG_PUT_PREV_TASK  = 1UL << 7,
+		DEBUG_CHECK_PREEMPT  = 1UL << 8,
+		DEBUG_RBTREE         = 1UL << 9,
+		DEBUG_TIMER          = 1UL << 10,
+		DEBUG_SUBMISSIONS    = 1UL << 11,
+		DEBUG_PICK_NEXT_TASK_FALLBACK = 1UL << 12,
+		DEBUG_ADAPT_SEXEC    = 1UL << 13,
+		DEBUG_SLACK_TIME     = 1UL << 14,
+};
+
+enum update_exec_time {
+	UPDATE_EXEC_TIME,
+	NO_UPDATE_EXEC_TIME,
+};
+
+#ifdef ATLAS_DEBUG
+//static const unsigned debug_mask = DEBUG_PICK_NEXT_TASK | DEBUG_PICK_NEXT_TASK_FALLBACK |
+//		DEBUG_PUT_PREV_TASK | DEBUG_SYS_SUBMIT | DEBUG_SYS_NEXT;
+
+static const unsigned debug_mask = 0;
+
+static int printk_counter = 0;
+	#define DEBUG(T,STR,...) \
+		do { \
+			if (T & debug_mask)  { \
+				preempt_disable(); \
+				printk_sched("%d (%d): "#T ": " STR "\n", (printk_counter++), \
+					smp_processor_id(), ##__VA_ARGS__); \
+				preempt_enable(); \
+			} \
+		} while(0)
+	
+	#define DEBUG_ON(T) if (debug_mask & (T))
+			
+#else 
+	#define DEBUG(...)
+	#define DEBUG_ON(T) if (0)
+#endif /* ATLAS_DEBUG */
+
+
+
+
+
+static inline void init_job(struct atlas_job *job) {
+	memset(job, 0, sizeof(struct atlas_job));
+	atomic_set(&job->count, 1);
+}
+
+static inline struct atlas_job *get_job
+	(struct atlas_job  *job)
+{
+	if (job)
+		atomic_inc(&job->count);
+	return job;
+}
+
+
+static void put_job(struct atlas_job *job)
+{
+	if (!job)
+		return;
+
+	if (atomic_dec_and_test(&job->count)) {
+		//printk_sched("free job=%p\n", job);
+		put_pid(job->pid);
+		kfree(job);
+	}
+}
+
+static inline int job_before(struct atlas_job *a,
+		struct atlas_job *b)
+{
+	BUG_ON(!a);
+	BUG_ON(!b);
+	return ktime_to_ns(a->deadline) <  ktime_to_ns(b->deadline);
+}
+
+static int entity_before(struct sched_atlas_entity *a,
+		struct sched_atlas_entity *b)
+{
+	
+	/*
+	 * a preemption within sys_next or a wakeup due to a signal can lead
+	 * into cases where se->job is null.
+	 * Because we also queue this se's into the tree, we have to check
+	 * both.
+	 * 
+	 * 4 cases:
+	 * new | comparator
+	 * ----------------
+	 *  o  |  o  doesn't matter
+	 *  o  |  x  new should go to the beginning
+	 *  x  |  o  the old entry should stay on the left side
+	 *  x  |  x  compare
+	 */
+	 
+	if (unlikely(!a->job)) //left side if new has no submisson
+		return 1;
+	
+	if (unlikely(!b->job)) //right side
+		return 0;
+		
+	return job_before(a->job, b->job);
+}
+
+static void enqueue_entity(struct atlas_rq *atlas_rq,
+		struct sched_atlas_entity *se)
+{
+	struct rb_node **link = &atlas_rq->tasks_timeline.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_atlas_entity *entry;
+	int leftmost = 1;
+	
+	//FIXME?
+	rb_init_node(&se->run_node);
+	
+	DEBUG(DEBUG_RBTREE, "enqueue_task_rb_tree");
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_atlas_entity, run_node);
+		
+		if (entity_before(se, entry))
+			link = &parent->rb_left;
+		else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		atlas_rq->rb_leftmost_se = &se->run_node;
+	
+	rb_link_node(&se->run_node, parent, link);
+	rb_insert_color(&se->run_node, &atlas_rq->tasks_timeline);	
+}
+
+static void dequeue_entity(struct atlas_rq *atlas_rq,
+		struct sched_atlas_entity *se)
+{
+	DEBUG(DEBUG_RBTREE, "dequeue_task_rb_tree");
+
+	if (atlas_rq->rb_leftmost_se == &se->run_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&se->run_node);
+		atlas_rq->rb_leftmost_se = next_node;
+	}
+	
+	rb_erase(&se->run_node, &atlas_rq->tasks_timeline);
+}
+
+static struct sched_atlas_entity *pick_first_entity(struct atlas_rq *atlas_rq)
+{
+	struct rb_node *left = atlas_rq->rb_leftmost_se;
+
+	if (!left)
+		return NULL;
+
+	return rb_entry(left, struct sched_atlas_entity, run_node);
+}
+
+static struct sched_atlas_entity *pick_next_entity(struct sched_atlas_entity *se)
+{
+	struct rb_node *next = rb_next(&se->run_node);
+
+	if (!next)
+		return NULL;
+
+	return rb_entry(next, struct sched_atlas_entity, run_node);
+}
+
+static struct atlas_job *pick_first_job(struct atlas_rq *atlas_rq) {
+	struct rb_node *first = rb_first(&atlas_rq->jobs);
+
+	if (!first)
+		return NULL;
+	
+	return rb_entry(first, struct atlas_job, rb_node);
+}
+
+static struct atlas_job *pick_last_job(struct atlas_rq *atlas_rq) {
+	struct rb_node *last = rb_last(&atlas_rq->jobs);
+
+	if (!last)
+		return NULL;
+	
+	return rb_entry(last, struct atlas_job, rb_node);
+}
+
+static struct atlas_job *pick_next_job(struct atlas_job *s) {
+	struct rb_node *next = rb_next(&s->rb_node);
+	
+	if (!next)
+		return NULL;
+	
+	return rb_entry(next, struct atlas_job, rb_node);
+}
+
+static struct atlas_job *pick_prev_job(struct atlas_job *s) {
+	struct rb_node *prev = rb_prev(&s->rb_node);
+	
+	if (!prev)
+		return NULL;
+	
+	return rb_entry(prev, struct atlas_job, rb_node);
+}
+
+static inline int job_in_rq(struct atlas_job *s) {
+	return !RB_EMPTY_NODE(&s->rb_node);
+}
+
+static inline struct task_struct *task_of_job(struct atlas_job *s) {
+	return get_pid_task(s->pid, PIDTYPE_PID);
+}
+
+static inline int in_slacktime(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME);
+}
+
+static inline int in_slacktime_cfs(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME_CFS);
+}
+
+static inline int in_slacktime_atlas(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME_ATLAS);
+}
+
+static inline ktime_t ktime_min(ktime_t a, ktime_t b) {
+	return ns_to_ktime(min(ktime_to_ns(a), ktime_to_ns(b)));
+}
+
+static inline int ktime_neg(ktime_t a) {
+	return ktime_to_ns(a) < 0;
+}
+
+static inline int ktime_zero(ktime_t a) {
+	return ktime_equal(ktime_set(0,0), a);
+}
+
+
+static ktime_t get_job_start(struct atlas_job *s) {
+	return ktime_sub(s->sdeadline, s->sexectime);
+}
+
+static inline int job_missed_deadline(struct atlas_job *s, ktime_t now) {
+	return ktime_neg(ktime_sub(s->sdeadline, now));
+}
+
+static inline int ktime_cmp(ktime_t a, ktime_t b) {
+	s64 tmp = ktime_to_ns(ktime_sub(a, b));
+	if (tmp > 0)
+		return 1;
+	else if (tmp == 0)
+		return 0;
+	else
+		return -1;
+}
+
+static inline struct rq *rq_of(struct atlas_rq *atlas_rq)
+{
+	return container_of(atlas_rq, struct rq, atlas);
+}
+
+static inline struct task_struct *task_of(struct sched_atlas_entity *se)
+{
+	return container_of(se, struct task_struct, atlas);
+}
+
+static inline void set_exhausted_exectime(struct sched_atlas_entity *se) {
+	se->flags |= ATLAS_EXHAUSTED_EXECTIME;
+}
+
+int
+hrtimer_start_nowakeup(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)
+{
+	return __hrtimer_start_range_ns(timer, tim, 0, mode, 0);
+}
+
+
+static void start_new_slack(struct atlas_rq *atlas_rq, ktime_t slack) {
+	ktime_t now, tmp, cfs_slice;
+	
+	now = atlas_rq->slack_timer.base->get_time();
+	atlas_rq->slack_end = ktime_add(now, slack);
+	
+	cfs_slice = ns_to_ktime(cfs_sched_period(rq_of(atlas_rq)->cfs.nr_running));
+	
+	tmp = ktime_min(cfs_slice, slack);
+	DEBUG(DEBUG_TIMER, "Slacktime up to: %lld",
+			ktime_to_us(atlas_rq->slack_end));
+
+	hrtimer_start_nowakeup(&atlas_rq->slack_timer, tmp, HRTIMER_MODE_REL_PINNED);
+	atlas_rq->flags |= SLACKTIME_CFS;
+}
+
+static void reset_slack_time(struct atlas_rq *atlas_rq) {
+	if (!(atlas_rq->flags & SLACKTIME))
+		return;
+	
+	hrtimer_cancel(&atlas_rq->slack_timer);
+	DEBUG(DEBUG_SLACK_TIME, "reset slack timer");
+	atlas_rq->flags &= ~SLACKTIME;
+}
+
+#ifdef ATLAS_DEBUG
+
+static struct sched_atlas_entity *pick_first_entity(struct atlas_rq *atlas_rq);
+static struct sched_atlas_entity *pick_next_entity(struct sched_atlas_entity *se);
+
+static void debug_job(struct atlas_job *s) {
+	if (!s) {
+		printk_sched("DEBUG_JOBS: NULL\n");
+		return;
+	}
+	printk_sched("DEBUG_JOBS: %6lld - %6lld (%6lld - %6lld) (%p, ref=%d)\n",
+		ktime_to_ms(ktime_sub(s->sdeadline, s->sexectime)),
+		ktime_to_ms(s->sdeadline),
+		ktime_to_ms(ktime_sub(s->deadline, s->exectime)),
+		ktime_to_ms(s->deadline),
+		s,
+		atomic_read(&s->count));
+}
+
+static void __debug_jobs(struct atlas_rq *atlas_rq) {
+	struct atlas_job *job, *prev = NULL;
+	
+	job = pick_first_job(atlas_rq);
+	printk_sched("DEBUG_JOBS:\n");
+	while (job) {
+		if (prev) {
+			ktime_t start, end, diff;
+			start = prev->sdeadline;
+			end = get_job_start(job);
+			diff = ktime_sub(end, start);
+			if (!ktime_zero(diff)) {
+				printk_sched("DEBUG_JOBS: %6lld - %6lld (gap=%lld)\n",
+				ktime_to_ms(start),
+				ktime_to_ms(end),
+				ktime_to_ms(diff));
+			}
+		}
+		debug_job(job);
+		prev = job;
+		job = pick_next_job(job);
+	}
+	printk_sched("======================\n");
+
+}
+
+static void debug_jobs(struct atlas_rq *atlas_rq) {
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	__debug_jobs(atlas_rq);
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+}
+/*
+ * rq must be locked
+ */
+static void __debug_rq(struct rq *rq) {
+	struct sched_atlas_entity *se;
+	
+	printk_sched("SCHED_ATLAS: DEBUG rq=%d\n", cpu_of(rq));
+	printk_sched("    Currently running: %d\n", rq->atlas.nr_runnable);
+	printk_sched("    Curr: pid=%d\n", rq->atlas.curr ? task_of(rq->atlas.curr)->pid : -1);
+	
+	printk_sched("    DEBUG tasks_timeline:\n");
+	se = pick_first_entity(&rq->atlas);
+	while (se) {
+		printk_sched("        pid=%5d, job=%p\n", task_of(se)->pid, se->job);
+		se = pick_next_entity(se);	
+	}
+	printk_sched("======================\n");
+	debug_jobs(&rq->atlas);
+}
+
+static void debug_rq(struct rq *rq) {
+	unsigned long flags;
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	__debug_rq(rq);
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+static void debug_task(struct task_struct *p) {
+	unsigned counter = 0;
+	struct atlas_job *job;
+	struct sched_atlas_entity *se = &p->atlas;
+	const char *s;
+	
+	printk_sched("SCHED_ATLAS: DEBUG task pid=%d\n", p->pid);
+	switch (p->atlas.state) {
+	case ATLAS_BLOCKED:
+		s = "ATLAS_BLOCKED";
+		break;
+	case ATLAS_UNDEF:
+		s = "ATLAS_UNDEF";
+		break;
+	case ATLAS_RUNNING:
+		s = "ATLAS_RUNNING";
+		break;
+	default:
+		s = "UNKNOWN";
+	}
+	
+	printk_sched("State: %s\n", s);
+	printk_sched("Submissions:\n");
+	spin_lock(&p->atlas.jobs_lock);
+	printk_sched("se->job=%p\n", p->atlas.job);
+	list_for_each_entry(job, &se->jobs, list) {
+		counter++;
+		debug_job(job);
+	}
+	printk_sched("    count: %d\n", counter);
+	printk_sched("======================\n");
+	spin_unlock(&p->atlas.jobs_lock);
+}
+#endif /* ATLAS_DEBUG */
+
+
+
+
+/*
+ * must be called with lock hold
+ */
+static void push_task_job(struct sched_atlas_entity *se,
+		struct atlas_job *new_job)
+{
+	struct list_head *entry;
+	struct atlas_job *job;
+
+	assert_spin_locked(&se->jobs_lock);
+
+	//typically, a new job should go to the end
+	//works also for empty list!
+	list_for_each_prev(entry, &se->jobs) {
+		job = list_entry(entry, struct atlas_job, list);
+		BUG_ON(!job);
+		if (job_before(job, new_job))
+			goto out;
+	}
+out:
+	list_add(&new_job->list, entry);
+}
+
+static struct atlas_job *pop_task_job(struct sched_atlas_entity *se)
+{
+	struct atlas_job *s = NULL;
+	struct list_head *elem;
+	
+	spin_lock(&se->jobs_lock);
+	
+	if (list_empty(&se->jobs))
+		goto out;
+	
+	elem = se->jobs.next;
+	s = list_entry(elem, struct atlas_job, list);
+	list_del(elem);
+out:
+	spin_unlock(&se->jobs_lock);
+	return s;
+}
+
+/* 
+ * must be called with rcu_read_lock hold
+ */
+static void assign_task_job(struct task_struct *p, struct atlas_job *job)
+{
+	struct sched_atlas_entity *se;
+	unsigned wakeup = 0;
+	
+
+	BUG_ON(!p);
+
+	{
+		//ensure that p is mapped to cpu 0
+		cpumask_t test;
+		cpumask_clear(&test);
+		cpumask_set_cpu(0, &test);
+
+		BUG_ON(!cpumask_equal(&test, &p->cpus_allowed));
+	}
+	
+	//new reference in list
+	get_job(job);
+	
+	se = &p->atlas;
+
+	spin_lock(&se->jobs_lock);
+	wakeup = list_empty(&se->jobs) && (se->state == ATLAS_BLOCKED);
+	push_task_job(se, job);
+	spin_unlock(&se->jobs_lock);
+	
+	/*
+	 * wake up process (we might wake up a process who does not wait for
+	 * the next job, but who cares...) 
+	 */
+	if (wakeup)
+		wake_up_process(p);
+}
+
+void erase_task_job(struct atlas_job *s) {
+	if (unlikely(!s))
+		return;
+	list_del(&s->list);
+	put_job(s);
+}
+
+/*******************************************************
+ * Scheduler stuff
+ */
+ 
+enum hrtimer_restart timer_rq_exec_func(struct hrtimer *timer);
+enum hrtimer_restart timer_rq_slack_func(struct hrtimer *timer);
+enum hrtimer_restart timer_rq_resched_func(struct hrtimer *timer);
+ 
+void init_atlas_rq(struct atlas_rq *atlas_rq)
+{
+	atlas_rq->curr = NULL;
+    atlas_rq->tasks_timeline = RB_ROOT;
+	atlas_rq->rb_leftmost_se = NULL;
+    atlas_rq->nr_runnable = 0;
+    printk(KERN_INFO "INIT_ATLAS_RUNQUEUE(%d): %p\n",
+		cpu_of(rq_of(atlas_rq)), atlas_rq);
+	atlas_rq->jobs = RB_ROOT;
+	hrtimer_init(&atlas_rq->exec_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	atlas_rq->exec_timer.function = &timer_rq_exec_func;
+	hrtimer_init(&atlas_rq->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	atlas_rq->slack_timer.function = &timer_rq_slack_func;
+	hrtimer_init(&atlas_rq->resched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	atlas_rq->resched_timer.function = &timer_rq_resched_func;
+	atlas_rq->flags = 0;
+}
+
+/*
+ * We are picking a new current task - update its stats:
+ */
+static inline void
+update_stats_curr_start(struct atlas_rq *atlas_rq, struct sched_atlas_entity *se)
+{
+	/*
+	 * We are starting a new run period:
+	 */
+	task_of(se)->se.exec_start = rq_of(atlas_rq)->clock_task;
+}
+
+static int update_execution_time(struct atlas_rq *atlas_rq,
+	struct atlas_job *job, ktime_t delta_exec);
+
+static void update_curr_atlas(struct rq *rq)
+{
+    //copied from rt
+	struct task_struct *curr = rq->curr;
+	struct sched_atlas_entity *se = &curr->atlas;
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	u64 delta_exec;
+	struct atlas_job *job = se->job;
+	unsigned long flags;
+
+	if (curr->sched_class != &atlas_sched_class)
+		return;
+
+	delta_exec = rq->clock_task - curr->se.exec_start;
+	if (unlikely((s64)delta_exec < 0))
+		delta_exec = 0;
+
+	schedstat_set(curr->se.statistics.exec_max, max(curr->se.statistics.exec_max, delta_exec));
+
+	curr->se.sum_exec_runtime += delta_exec;
+	account_group_exec_runtime(curr, delta_exec);
+
+	update_stats_curr_start(atlas_rq, se);
+	cpuacct_charge(curr, delta_exec);
+	
+	//we are going to change the job tree, so lock it
+	//UPDTAE: if we are called because of a scheduling tick we are in interrupt
+	//context and have to skip the update because the lock might not be free!!
+
+	/*
+	 * when kworker is woken up, we are in interrupt context,
+	 * but we want to update the plan of execution
+	 */
+	//if (in_interrupt())
+	//	return;
+
+	/*
+	 * do not update execution plan if there is no job or
+	 * if our selection was not based on LRT
+	 */
+	if (unlikely(!job) || (se->flags & (ATLAS_FALLBACK | ATLAS_SLACK)))
+		return;
+
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	update_execution_time(atlas_rq, job, ns_to_ktime(delta_exec)); 
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+}
+
+
+
+/*
+ * the new task may be enqueued with the ATLAS_MISSED_DEADLINE bit set
+ * 
+ * -> in this case, the resched bit should has been set by the timer routine
+ * 
+ */
+static void enqueue_task_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &p->atlas;
+	
+	WARN_ON((se->flags & ATLAS_MISSED_DEADLINE) && !(test_tsk_need_resched(p)));
+	
+	DEBUG(DEBUG_ENQUEUE, "p->pid=%d curr->pid=%d atlas->curr->pid=%d", p->pid,
+		rq->curr ? rq->curr->pid : -1,
+		rq->atlas.curr ? task_of(rq->atlas.curr)->pid : -1);
+	
+	if (atlas_rq->curr != se)
+		enqueue_entity(atlas_rq, se);
+    
+    //mark task as on runqueue now
+	se->on_rq = 1;
+    atlas_rq->nr_runnable++;
+    
+    inc_nr_running(rq);
+
+	/*
+	 * we cannot rely on the calculated slack time
+	 * because it depends on the previous running task
+	 *
+	 * do only reset_slack_time if current is not scheduled
+	 * by atlas, otherwise it is done in check_preempt_atlas
+	 */
+	if (current != task_of(atlas_rq->curr))
+		reset_slack_time(atlas_rq);
+	
+
+	return;
+}
+
+
+/*
+ * also called when switching to CFS because of missed deadline
+ */
+static void dequeue_task_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &p->atlas;
+	
+	DEBUG(DEBUG_DEQUEUE, "pid=%d", p ? p->pid : -1);
+	
+	update_curr_atlas(rq);	
+
+    if (atlas_rq->curr == se)
+		atlas_rq->curr = NULL;
+	else
+		dequeue_entity(atlas_rq, se);
+	
+	se->on_rq = 0;
+	
+    atlas_rq->nr_runnable--;
+
+	if (atlas_rq->nr_runnable == 0)
+		reset_slack_time(atlas_rq);
+
+    dec_nr_running(rq);
+	return;
+}
+
+static void yield_task_atlas(struct rq *rq)
+{
+    return;
+}
+
+static void check_preempt_curr_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_atlas_entity *se = &curr->atlas, *pse = &p->atlas;
+	int sub = (se->job != NULL), psub = (pse->job != NULL);
+	
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d", p->pid);
+	
+	if (unlikely(se == pse)) {
+		DEBUG(DEBUG_CHECK_PREEMPT, "se == pse; pid=%d don't preempt curr->pid=%d",
+			p->pid, curr->pid);
+		return;
+	}
+	
+	if (test_tsk_need_resched(curr)) {
+		DEBUG(DEBUG_CHECK_PREEMPT, "test_tsk_need_resched; pid=%d don't preempt curr->pid=%d",
+			p->pid, curr->pid);
+		return;
+	}
+	
+		
+	/* Idle tasks are by definition preempted by non-idle tasks. */
+	if (unlikely(curr->policy == SCHED_IDLE) &&
+	    likely(p->policy != SCHED_IDLE))
+		goto preempt;
+	
+	/* Preempt tasks scheduled by cfs */
+	if (likely(curr->policy == SCHED_NORMAL))
+		goto preempt;
+	
+	/* Bug if task is not scheduled by us */
+	BUG_ON(curr->policy != SCHED_ATLAS);
+	
+	/* if the new task has no job, preempt */
+	if (unlikely(!psub))
+		goto preempt;
+	
+	/* if the currently running task has no job, don't preempt */
+	if (unlikely(!sub))
+		goto no_preempt;
+		
+	if (job_before(pse->job, se->job))
+		goto preempt;
+	
+no_preempt:
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d don't preempt curr->pid=%d",
+		p->pid, curr->pid);
+
+	return;
+	
+preempt:
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d preempt curr->pid=%d",
+		p->pid, curr->pid);
+	reset_slack_time(&rq->atlas);
+	resched_task(curr);
+
+	return;
+}
+
+static int get_slacktime(struct atlas_rq *atlas_rq, ktime_t *slack);
+static void cleanup_rq(struct atlas_rq *atlas_rq, ktime_t ktime);
+static inline void setup_rq_exec_timer(struct atlas_rq *atlas_rq,
+		struct atlas_job *s);
+static inline void setup_rq_resched_timer(struct atlas_rq *atlas_rq,
+		ktime_t ktime);
+
+static struct task_struct *pick_next_task_atlas(struct rq *rq)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se;
+	ktime_t slack;
+	struct atlas_job *s, *s_next;
+	struct task_struct *p;
+	unsigned long flags;
+	int exec_timer = 1;
+
+	/*
+	 * the resched_timer might be setup because of the job of
+	 * task that has moved to CFS (no slack time)
+	 */
+	hrtimer_cancel(&atlas_rq->resched_timer);
+
+	/*
+	 * only proceed if there are runnable tasks
+	 */
+	if (likely(!atlas_rq->nr_runnable)) {
+		//if there is no ready task, no need to set up timer
+		return NULL;
+	}
+
+	BUG_ON(atlas_rq->curr);
+	
+	se = pick_first_entity(atlas_rq);
+
+	/*
+	 * jobs without a job are doing signal handling
+	 */
+	if (unlikely(!se->job)) {
+		atlas_rq->curr = se;
+		dequeue_entity(atlas_rq, se);
+		goto out;
+	}
+	
+	/*
+	 * slack time reserved for CFS?
+	 */
+	if (in_slacktime_cfs(atlas_rq))
+		return NULL;
+	
+	/*
+	 * slacktime for ATLAS?
+	 */
+	if (in_slacktime_atlas(atlas_rq)) {
+		se->flags |= ATLAS_SLACK; 
+		atlas_rq->curr = se;
+		dequeue_entity(atlas_rq, se);
+		/* no accounting of execution time, so don't setup timer */
+		exec_timer = 0;
+		goto out;
+	}
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	
+	/*
+	 * remove jobs having a deadline in the past
+	 */
+	//cleanup_rq(atlas_rq, ktime_get());
+	
+	/*
+	 * job of se might be already removed from the rq because of cleanup
+	 * -> move the task to CFS (it is rescheduled again, so we are called again)
+	 */
+	if (unlikely(!job_in_rq(se->job))) {
+		BUG();
+		se->flags |= ATLAS_MISSED_DEADLINE;
+		resched_task(task_of(se));
+		atlas_rq->curr = se;
+		dequeue_entity(atlas_rq, se);
+		goto unlock_out;
+	}
+
+	//check if there is slack time left
+	if (get_slacktime(atlas_rq, &slack))
+	{
+		start_new_slack(atlas_rq, slack);
+		//start new slack time, return NULL, because CFS will start anyway
+		goto unlock_out;
+	}
+
+	DEBUG(DEBUG_PICK_NEXT_TASK, "no slack time");
+	//no slack time
+	s = se->job;
+	BUG_ON(s == NULL);
+	BUG_ON(!job_in_rq(s));
+
+	s_next = pick_first_job(atlas_rq);
+	BUG_ON(s_next == NULL);
+	while (s != s_next) {
+		if (!ktime_zero(s_next->sexectime)) {
+			p = task_of_job(s_next);
+			if (p && p->policy != SCHED_ATLAS) {
+				/*
+				 * the job's thread is not scheduled by us,
+				 * but we provided some time -> give it to CFS
+				 * 
+				 * we use the reschedule timer
+				 */
+				setup_rq_resched_timer(atlas_rq, s_next->sdeadline);
+				exec_timer = 0;
+				goto unlock_out;
+			}
+		}
+		/*
+		 * s_next is a job of a blocked thread handled by our
+		 * scheduler, so we go on with the next job in admission plan
+		 */
+		s_next = pick_next_job(s_next);
+		BUG_ON(s_next == NULL);
+	}
+
+	atlas_rq->curr = se;
+	dequeue_entity(atlas_rq, se);
+
+unlock_out:
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+
+out:	
+	if (atlas_rq->curr) {
+		DEBUG(DEBUG_PICK_NEXT_TASK, "pid=%d, need_resched=%d",
+			task_of(atlas_rq->curr)->pid, test_tsk_need_resched(task_of(atlas_rq->curr)));
+		update_stats_curr_start(atlas_rq, atlas_rq->curr);
+		if (exec_timer)
+			setup_rq_exec_timer(atlas_rq, atlas_rq->curr->job);
+    	return task_of(atlas_rq->curr);
+	} else {
+		DEBUG(DEBUG_PICK_NEXT_TASK, "NULL");
+		return NULL;
+	}
+}
+
+static struct task_struct *pick_next_task_atlas_fallback(struct rq *rq)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se;
+	
+	if (likely(!atlas_rq->nr_runnable)) {
+		return NULL;
+	}
+
+	BUG_ON(atlas_rq->curr);
+	//if there are running tasks, we are processing in slack_time
+	BUG_ON(!in_slacktime(atlas_rq));
+	
+	se = pick_first_entity(atlas_rq);
+	atlas_rq->curr = se;
+	dequeue_entity(atlas_rq, se);
+	
+	update_stats_curr_start(atlas_rq, se);
+
+	//mark as fallback decision, needed by check_preempt_current
+	se->flags |= ATLAS_FALLBACK;
+	
+	BUG_ON(!se->job);
+	BUG_ON(ktime_neg(se->job->sexectime));
+	if (unlikely(ktime_zero(se->job->sexectime))) {
+		set_exhausted_exectime(se);
+		set_tsk_need_resched(task_of(se));
+	} else {
+		hrtimer_start_nowakeup(&atlas_rq->exec_timer, se->job->sexectime,
+			HRTIMER_MODE_REL_PINNED);	
+	}
+	
+	DEBUG(DEBUG_PICK_NEXT_TASK_FALLBACK, "pid=%d", task_of(se)->pid);
+	return task_of(se);
+}
+
+static void put_prev_task_atlas(struct rq *rq, struct task_struct *prev)
+{	
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &prev->atlas;
+	
+	DEBUG(DEBUG_PUT_PREV_TASK, "pid=%d (on_rq=%d, missed_deadline=%lu, fallback=%lu)", prev->pid,
+		se->on_rq, se->flags & ATLAS_MISSED_DEADLINE,
+		se->flags & ATLAS_FALLBACK);
+	
+	hrtimer_cancel(&rq->atlas.exec_timer);
+
+	if (se->on_rq) {
+		update_curr_atlas(rq);
+		enqueue_entity(atlas_rq, se);
+	}
+	
+	se->flags &= ~(ATLAS_FALLBACK | ATLAS_SLACK);
+	rq->atlas.curr = NULL;
+
+	return;
+}
+
+static void set_curr_task_atlas(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+	struct sched_atlas_entity *se = &p->atlas;
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	
+	DEBUG(DEBUG_SET_CURR_TASK, "pid=%d", p->pid);
+    update_stats_curr_start(atlas_rq, se); 
+    
+    WARN_ON(rq->atlas.curr);
+	rq->atlas.curr = se;
+
+	
+	/*
+	 * reset slack timer and reschedule
+	 */
+	reset_slack_time(atlas_rq);
+	resched_task(p);
+   	
+    return;
+}
+
+static void task_tick_atlas(struct rq *rq, struct task_struct *p, int queued)
+{
+    update_curr_atlas(rq);
+    return;
+}
+
+static void prio_changed_atlas(struct rq *rq, struct task_struct *p, int oldprio)
+{
+    //printk(KERN_INFO "SCHED_ATLAS: prio_changed\n");
+    return;
+}
+
+static void switched_from_atlas(struct rq *rq, struct task_struct *p)
+{
+    //printk(KERN_INFO "SCHED_ATLAS: switched_from\n");
+    return;
+}
+
+static void switched_to_atlas(struct rq *rq, struct task_struct *p)
+{
+    DEBUG(DEBUG_SWITCHED_TO, "pid=%d", p->pid);
+    return;
+}   
+
+static unsigned int get_rr_interval_atlas(struct rq *rq, struct task_struct *task)
+{
+    printk(KERN_INFO "SCHED_ATLAS: get_rr_interval\n");
+    return 0;
+}
+
+#ifdef CONFIG_SMP
+static int select_task_rq_atlas(struct task_struct *p, int sd_flag, int flags)
+{
+    return task_cpu(p);
+    
+}
+#endif /* CONFIG_SMP */
+ 
+ 
+ 
+/**
+ * timer related stuff
+ */
+
+/*
+ * called when a process missed its deadline; called from irq context
+ */
+enum hrtimer_restart atlas_timer_task_function(struct hrtimer *timer)
+{
+	struct sched_atlas_entity *se = container_of(timer, struct sched_atlas_entity, timer);
+	struct task_struct *p = task_of(se);
+
+	/*
+	 * optimization: if se->job == NULL the task is in between sys_next
+	 * and there is no need to set the flag.
+	 */
+	
+	WARN_ON(!se->job);
+	//se->flags |= ATLAS_MISSED_DEADLINE;
+	DEBUG(DEBUG_TIMER, "deadline missed: pid=%d", task_of(se)->pid);
+	wmb();
+	set_tsk_need_resched(p);
+	//send_sig(SIGXCPU, p, 0);	
+	
+	return HRTIMER_NORESTART;
+}
+
+enum hrtimer_restart timer_rq_exec_func(struct hrtimer *timer)
+{
+	struct atlas_rq *atlas_rq = container_of(timer, struct atlas_rq, exec_timer);
+	struct rq *rq = rq_of(atlas_rq);
+	unsigned long flags;
+	
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (rq->curr) {
+		WARN_ON(rq->curr != task_of(atlas_rq->curr));
+		set_exhausted_exectime(&rq->curr->atlas);
+		resched_task(rq->curr);
+	}
+	DEBUG(DEBUG_TIMER, "exec_func, resched pid=%d", rq->curr ? rq->curr->pid : -1);
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	return HRTIMER_NORESTART;
+}
+
+static inline void setup_rq_exec_timer(struct atlas_rq *atlas_rq,
+		struct atlas_job *s) {
+
+	if (unlikely(!s))
+		return;
+
+	BUG_ON(ktime_neg(s->sexectime));
+	BUG_ON(ktime_zero(s->sexectime) && !atlas_rq->curr);
+
+	if (unlikely(ktime_zero(s->sexectime))) {
+		set_exhausted_exectime(atlas_rq->curr);
+		resched_task(task_of(atlas_rq->curr));
+	} else {
+		hrtimer_start_nowakeup(&atlas_rq->exec_timer,
+			s->sexectime, HRTIMER_MODE_REL_PINNED);
+	}
+}
+
+static inline void setup_rq_resched_timer(struct atlas_rq *atlas_rq, ktime_t ktime) {
+	hrtimer_start_nowakeup(&atlas_rq->exec_timer, ktime, HRTIMER_MODE_ABS_PINNED);
+}
+
+/*
+ * handle slack time transitions
+ */
+enum hrtimer_restart timer_rq_slack_func(struct hrtimer *timer)
+{
+	struct atlas_rq *atlas_rq = container_of(timer, struct atlas_rq, slack_timer);
+	struct rq *rq = rq_of(atlas_rq);
+	unsigned long flags;
+	ktime_t now, remaining, slack;
+	
+	now = timer->base->get_time();
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+
+	BUG_ON(!(atlas_rq->flags & SLACKTIME));
+	
+	remaining = ktime_sub(atlas_rq->slack_end, now);
+	if (ktime_cmp(remaining, ns_to_ktime(sysctl_sched_atlas_min_slack)) <= 0) {
+		//no more slack time left
+		atlas_rq->flags &= ~SLACKTIME;
+		DEBUG(DEBUG_TIMER, "slack_func: no more slack time left");
+		
+		/* resched curr */
+		if (rq->curr)
+			resched_task(rq->curr);
+		
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+		return HRTIMER_NORESTART;
+	}
+
+	if (atlas_rq->flags & SLACKTIME_CFS) {
+		ktime_t atlas_slice = ns_to_ktime(sysctl_sched_atlas_slice);
+		//it's time for atlas
+		atlas_rq->flags &= ~SLACKTIME;
+		atlas_rq->flags |= SLACKTIME_ATLAS;
+		
+		slack = ktime_min(remaining, atlas_slice);
+	} else {
+		ktime_t cfs_slice = ns_to_ktime(cfs_sched_period(rq->cfs.nr_running));
+		//time for cfs
+		atlas_rq->flags &= ~SLACKTIME;
+		atlas_rq->flags |= SLACKTIME_CFS;
+	
+		slack = ktime_min(remaining, cfs_slice);
+	}
+	hrtimer_forward_now(timer, slack);
+	//in case timer was programmed for slack time
+
+	if (rq->curr)
+		resched_task(rq->curr);
+
+	DEBUG(DEBUG_TIMER, "slack_func: switch to %s, resched pid=%d, slack=%llu",
+		atlas_rq->flags & SLACKTIME_CFS ? "CFS" : "ATLAS",
+		rq->curr ? rq->curr->pid : -1,
+		ktime_to_ms(slack));
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	return HRTIMER_RESTART;
+}
+
+enum hrtimer_restart timer_rq_resched_func(struct hrtimer *timer)
+{
+	struct atlas_rq *atlas_rq = container_of(timer, struct atlas_rq, slack_timer);
+	struct rq *rq = rq_of(atlas_rq);
+	
+	if (rq->curr)
+		set_tsk_need_resched(rq->curr);
+
+	return HRTIMER_NORESTART;
+}
+
+/*
+ * Methods to maintain job tree.
+ */
+
+static inline int is_collision(struct atlas_job *a, struct atlas_job *b) {
+	ktime_t b_start = get_job_start(b);
+	ktime_t a_end = a->sdeadline;
+	if (ktime_cmp(a_end, b_start) == 1) {
+		//end > start
+		return 1;
+	}
+	return 0;
+}
+
+static void check_admission_plan(struct atlas_rq *atlas_rq) {
+#ifdef DEBUG
+	struct atlas_job *prev, *next;
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	//__debug_jobs(atlas_rq);
+	
+	prev = pick_first_job(atlas_rq);
+
+	if (!prev)
+		return;
+
+	while ((next = pick_next_job(prev))) {
+		if (is_collision(prev, next)) {
+			BUG();
+		}
+		prev = next;
+	}
+#endif
+}
+
+static inline void resolve_collision(struct atlas_job *a,
+		struct atlas_job *b) {
+	//assumption: there is a collision
+	a->sdeadline = get_job_start(b);
+}
+
+/*
+ * close the gap between job a and b and
+ * return 1 iff start of job a was moved forward
+ */
+static inline int collapse_jobs(struct atlas_job *a,
+		struct atlas_job *b, enum update_exec_time update) {
+	
+	ktime_t start_a, start_b, end, move;
+	//can we move job a forward? if not, we are ready
+	if (likely(ktime_equal(a->deadline, a->sdeadline)))
+		return 0;
+	
+	//adapt the deadline of the job
+	start_a = get_job_start(a);
+	start_b = get_job_start(b);
+	end = ktime_min(a->deadline, start_b);
+
+	//end is either the start of the next job or the real deadline
+	
+	//save the movement
+	move = ktime_sub(end, a->sdeadline);
+	a->sdeadline = end;
+	
+	//no update of execution time possible/allowed?
+	if (update == NO_UPDATE_EXEC_TIME ||
+			likely(ktime_equal(a->exectime, a->sexectime))) {
+		//we moved the start
+		return 1;
+	}
+
+	//extend the execution time
+	a->sexectime = ktime_min(a->exectime, ktime_add(move, a->sexectime));
+
+	//did we moved the start?
+	if (ktime_equal(start_a, get_job_start(a))) {
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * close gaps, called when a job is removed or when its exec time was updated
+ *
+ * note: - whenever updating the job's execution time,
+ *         the wall clock time moves also forward. It's therefore
+ *         illegal to extend the execution time of the previous jobs,
+ *         otherwise exection time would be created that isn't available
+ *       - it is completely admissible to extract the execution time
+ *         of previous jobs whenever a job is removed from the execution
+ *         plan
+ */
+static inline void close_gaps(struct atlas_job *job, enum update_exec_time update) {
+	struct atlas_job *prev;
+	while((prev = pick_prev_job(job))) {
+		if (!collapse_jobs(prev, job, update))
+			break;
+		job = prev;
+	}
+
+}
+
+/*
+ * calculate the gap between two jobs
+ */
+static inline ktime_t calc_gap(struct atlas_job *a, struct atlas_job *b) {
+	ktime_t start = get_job_start(b);
+	ktime_t ret = ktime_sub(start, a->sdeadline);
+
+	BUG_ON(ktime_to_ns(ret) < 0);
+	return ret;
+}
+
+static inline void erase_rq_job(struct atlas_rq *atlas_rq,
+		struct atlas_job *job);
+
+/*
+ * must be called with atlas_rq locked
+ */
+static void assign_rq_job(struct atlas_rq *atlas_rq,
+		struct atlas_job *job, ktime_t now) {
+	
+	struct rb_node **link;
+	struct rb_node *parent = NULL;
+	struct atlas_job *entry, *next, *prev;
+	ktime_t start_first;	
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	//cleanup_rq(atlas_rq, now);
+	
+	link = &atlas_rq->jobs.rb_node;
+	
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct atlas_job, rb_node);
+		
+		if (job_before(job, entry))
+			link = &parent->rb_left;
+		else
+			link = &parent->rb_right;
+	}
+	
+	rb_link_node(&job->rb_node, parent, link);
+	rb_insert_color(&job->rb_node, &atlas_rq->jobs);	
+	
+	//save reference
+	get_job(job);
+
+	/*
+	 * the new submission is in place
+	 */
+
+	/* fix the scheduled deadline of the new job*/
+	next = pick_next_job(job);
+	if (next && is_collision(job, next)) {
+		resolve_collision(job, next);
+	}
+
+	/*
+	 * FIXME: the scheduled deadline might be in the past?!
+	 *        -> does it makes sense? for the moment
+	 *           we remove the job, because we cannot calculate
+	 *           a reasonable sexectime
+	 */
+
+	/*if (ktime_cmp(now, job->sdeadline) == 1) { /* now > sdeadline */
+	/*	//the job should be the first one
+		BUG_ON(job != pick_first_job(atlas_rq));
+		erase_rq_job(atlas_rq, job);
+		goto out;
+	}*/
+
+	/* fix scheduled execution time */
+	
+	next = pick_first_job(atlas_rq);
+	start_first = get_job_start(next);
+	/*if (next == job) {
+		ktime_t diff = ktime_sub(job->sdeadline, now);
+		job->sexectime = ktime_min(diff, job->sexectime);
+	} else {
+		ktime_t max_exec = ktime_sub(job->sdeadline, now);
+		
+		//take care of the first job if now > start
+		//in this case we substract to much later on
+		ktime_t start = get_job_start(next);
+		if (ktime_cmp(now, start) == 1) {
+			ktime_t diff = ktime_sub(now, start);
+			max_exec = ktime_add(max_exec, diff);
+		}
+
+		while (next != job) {
+			max_exec = ktime_sub(max_exec, next->sexectime);
+			next = pick_next_job(next);
+		}
+
+		if (ktime_neg(max_exec)) {
+			job->sexectime = ktime_set(0,0);
+		} else {
+			job->sexectime = ktime_min(job->sexectime, max_exec);
+		}
+	}*/
+
+	/*
+	 * update the scheduled deadlines of the jobs placed before
+	 * the new job
+	 */
+	while ((prev = pick_prev_job(job))) {
+		if (!is_collision(prev, job))
+			break;
+		resolve_collision(prev, job);
+		job = prev;
+	}
+
+	/*
+	 * reset slack time iff start moved to the left
+	 *   - we have to initiate a reschedule on the target cpu
+	 */
+	if (ktime_cmp(start_first,
+			get_job_start(pick_first_job(atlas_rq)))) {
+		resched_cpu(cpu_of(rq_of(atlas_rq)));	
+	}
+
+out:
+	check_admission_plan(atlas_rq);
+}
+
+static int update_execution_time(struct atlas_rq *atlas_rq,
+	struct atlas_job *job, ktime_t delta_exec) {
+	
+	int ret = 0;
+
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	job->exectime = ktime_sub(job->exectime, delta_exec); 
+
+	if (unlikely(ktime_neg(job->exectime))) {
+		job->exectime = ktime_set(0,0);
+		job->sexectime = ktime_set(0,0);
+		ret = 1;
+		goto out;
+	}
+
+	job->sexectime = ktime_sub(job->sexectime, delta_exec);
+	if (ktime_neg(job->sexectime)) {
+		job->sexectime = ktime_set(0,0);
+		ret = 2;
+	}
+
+out:
+	//adapt admission plan
+	close_gaps(job, NO_UPDATE_EXEC_TIME);
+
+	check_admission_plan(atlas_rq);   
+	
+	return ret;
+}
+
+/*
+ * atlas_rq->lock must be hold!
+ */
+static inline void erase_rq_job(struct atlas_rq *atlas_rq,
+		struct atlas_job *job)
+{	
+	// a job is removed from the rq from next and also in
+	// pick_next_task on cleanup, so there is a race condition
+	if (unlikely(!job) ||
+			unlikely(!job_in_rq(job)))
+		return;
+		
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	if (likely(job_in_rq(job))) {
+		job->sexectime = ktime_set(0,0);
+		close_gaps(job, UPDATE_EXEC_TIME);
+		rb_erase(&job->rb_node, &atlas_rq->jobs);
+		RB_CLEAR_NODE(&job->rb_node);
+		put_job(job);
+	}	
+
+	check_admission_plan(atlas_rq);
+}
+
+/*
+ * determine if there is slack time left. job tree has to be locked
+ * 
+ * return: 1 if there is slack time
+ * 		   0 if there is no slack time
+ * 
+ * the amount of slack time left is returned in ktime
+ */
+static int get_slacktime(struct atlas_rq *atlas_rq, ktime_t *slack) {
+	struct sched_atlas_entity *se;
+	struct atlas_job *job;
+	ktime_t start, sum, now;
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	se = pick_first_entity(atlas_rq);
+	BUG_ON(!se);
+	BUG_ON(!se->job);
+	BUG_ON(!job_in_rq(se->job));
+	
+	job = se->job;
+	//get start point
+	start = get_job_start(job);
+	
+	//sum up the execution time of the jobs before
+	sum = ktime_set(0,0);
+	while((job = pick_prev_job(job))) {
+		sum = ktime_add(sum, job->sexectime);
+	}
+
+	now = atlas_rq->slack_timer.base->get_time();
+	*slack = ktime_sub(ktime_sub(start, now), sum);
+
+	if (ktime_to_ns(*slack) > sysctl_sched_atlas_min_slack)
+		return 1;
+	else
+		return 0;
+}
+
+static void cleanup_rq(struct atlas_rq *atlas_rq, ktime_t now) {
+	struct atlas_job *tmp, *s = pick_first_job(atlas_rq);
+
+	assert_raw_spin_locked(&atlas_rq->lock);
+	while (s && unlikely(job_missed_deadline(s, now))) {
+		/*struct task_struct *p = task_of_job(s);
+		
+		if (p) {
+			printk_sched("drop Submission from rq; sub=%p pid=%d scheduler=%d sub_task=%p\n",
+					s, p->pid, p->policy, p->atlas.job);
+		} else {
+			printk_sched("drop Submission of nonexistent task from rq; sub=%p\n", s);
+		}*/
+
+		tmp = s;
+		s = pick_next_job(s);
+		erase_rq_job(atlas_rq, tmp);
+	}
+}
+
+/* 
+ * free pending jobs of a killed task
+ * called from do_exit()
+ *
+ * there might also be the timer
+ */
+void exit_atlas(struct task_struct *p) {
+	struct atlas_job *job, *tmp;
+	struct rq *rq = task_rq(p);
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	unsigned long flags;
+	
+	if (cpu_of(rq) != 0)
+		return;
+
+	hrtimer_cancel(&p->atlas.timer);
+	
+	//debug_rq(rq);
+	//debug_task(p);
+
+	BUG_ON(in_interrupt());
+	//remove jobs from run queue
+	if ((job = p->atlas.job)) {
+		p->atlas.job = NULL;
+		put_job(job);
+		raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+		erase_rq_job(atlas_rq, job);
+		raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+	}
+	
+	spin_lock(&p->atlas.jobs_lock);
+	list_for_each_entry_safe(job, tmp, &p->atlas.jobs, list) {
+		raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+		erase_rq_job(atlas_rq, job);
+		raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+
+		erase_task_job(job);
+	}
+	spin_unlock(&p->atlas.jobs_lock);
+
+	//debug_rq(rq);
+	//debug_task(p);
+}
+
+
+
+
+/*
+ * All the scheduling class methods:
+ */
+const struct sched_class atlas_sched_class = {
+	.next               = &fair_sched_class,
+	.enqueue_task       = enqueue_task_atlas,
+	.dequeue_task       = dequeue_task_atlas,
+	.yield_task         = yield_task_atlas,
+	//.yield_to_task		= yield_to_task_atlas,
+
+	.check_preempt_curr = check_preempt_curr_atlas,
+
+	.pick_next_task     = pick_next_task_atlas,
+	.put_prev_task      = put_prev_task_atlas,
+
+/**we do not support SMP so far*/
+#ifdef CONFIG_SMP
+	.select_task_rq     = select_task_rq_atlas,
+
+	//.rq_online		= rq_online_atlas,
+	//.rq_offline		= rq_offline_atlas,
+
+	//.task_waking		= task_waking_atlas,
+#endif
+
+	.set_curr_task      = set_curr_task_atlas,
+	.task_tick          = task_tick_atlas,
+	//.task_fork        = task_fork_atlas,
+
+	.prio_changed       = prio_changed_atlas,
+	.switched_from      = switched_from_atlas,
+	.switched_to        = switched_to_atlas,
+
+	.get_rr_interval    = get_rr_interval_atlas,
+
+};
+
+
+
+const struct sched_class atlas_fallback_sched_class = {
+	.next               = &idle_sched_class,
+	.pick_next_task     = pick_next_task_atlas_fallback,
+};
+
+
+#ifdef ATLAS_DEBUG
+#define OP_DEBUG_RUNQUEUE 1
+#define OP_UPDATE_EXECTIME 2
+#define OP_DELETE_JOB 3
+#endif
+
+SYSCALL_DEFINE3(atlas_debug, int, operation, int, arg1, int, arg2)
+{
+#ifdef ATLAS_DEBUG
+	struct rq *rq = &per_cpu(runqueues, 0);
+	switch (operation) {
+		case OP_DEBUG_RUNQUEUE:
+			debug_rq(rq);
+			break;
+		case OP_UPDATE_EXECTIME: {
+			struct atlas_rq *atlas_rq = &rq->atlas;
+			struct atlas_job *job;
+			int nr_job = arg1;
+			unsigned long flags;
+
+			s64 delta_exec = arg2 * 1000 * 1000; /*ns*/	
+
+			raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+			job = pick_first_job(atlas_rq);
+			while (job && nr_job) {
+				job = pick_next_job(job);
+				nr_job--;
+			}
+			if (job)
+				update_execution_time(atlas_rq, job, ns_to_ktime(delta_exec)); 
+			raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+			break;
+		}
+		case OP_DELETE_JOB: {
+			struct atlas_rq *atlas_rq = &rq->atlas;
+			struct atlas_job *job;
+			int nr_job = arg1;
+			unsigned long flags;
+
+			raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+			job = pick_first_job(atlas_rq);
+			while (job && nr_job) {
+				job = pick_next_job(job);
+				nr_job--;
+			}
+			if (job)
+				erase_rq_job(atlas_rq, job);
+			raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+			break;
+		}
+		default:
+			break;
+	}
+#endif
+	return 0;
+}
+
+/*
+ * sys_atlas_next
+ */
+SYSCALL_DEFINE0(atlas_next)
+{
+	int ret = 0, missed = 0;
+	struct atlas_job *old_job;
+	struct sched_atlas_entity *se = &current->atlas;
+	struct rq *rq;
+	struct atlas_rq *atlas_rq;
+	unsigned long flags;
+	ktime_t now;
+
+	//printk(KERN_INFO "##START## pid=%d policy=%s job=%p\n", current->pid,
+	//	current->policy == SCHED_NORMAL ? "CFS" :
+	//	current->policy == SCHED_ATLAS  ? "ATLAS" : "UNKNOWN",
+	//	se->job);
+
+	/*
+	 * not necessary to check if the timer's callback run,
+	 * because we are already scheduled by CFS (scheduler
+	 * runs when returning from interrupt) in this case
+	 */
+	hrtimer_cancel(&se->timer);
+	se->flags &= ~ATLAS_MISSED_DEADLINE;
+
+	//we are done with the old job
+	old_job = se->job;
+	se->job = NULL;
+	put_job(old_job);
+	
+	/*
+	 * No preemption in favour of another job, but timer interrupt
+	 * may switch our scheduler because of an exceeded exectime.
+	 */
+	preempt_disable();
+	
+	rq = task_rq(current);
+	atlas_rq = &rq->atlas;
+
+	/*
+	 * reset exec_timer and flag for missed exectime
+	 */
+	hrtimer_cancel(&atlas_rq->exec_timer);
+	se->flags &= ~ATLAS_EXHAUSTED_EXECTIME;
+	
+	//remove the old job from the rq
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (current->policy == SCHED_ATLAS)
+		update_curr_atlas(rq);
+
+	raw_spin_lock(&atlas_rq->lock);
+	erase_rq_job(atlas_rq, old_job);
+	raw_spin_unlock(&atlas_rq->lock);
+
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+
+	//get new job
+	se->job = pop_task_job(se);
+
+	//deadline of the next job already missed?
+	/*if (se->job) {
+		now = ktime_get();
+		if (ktime_cmp(se->job->deadline, now) < 0)
+			missed = 1;
+	}*/
+	
+	preempt_enable();
+
+	//WARN_ON(missed && current->policy == SCHED_ATLAS);
+
+	/*if (missed && current->policy == SCHED_NORMAL) {
+		printk(KERN_INFO "next(): pid=%d: already missed next deadline, "
+			"don't switch back to ATLAS\n"
+			"now=%llu deadline=%llu\n", current->pid,
+			ktime_to_ns(now),
+			ktime_to_ns(se->job->deadline));
+	}*/
+
+	
+	// switch to SCHED_ATLAS when not missed deadline
+	//if (!missed && current->policy == SCHED_NORMAL) {
+	if (current->policy == SCHED_NORMAL) {
+		struct sched_param lparam = {0};
+		sched_setscheduler_nocheck(current, SCHED_ATLAS | SCHED_RESET_ON_FORK, &lparam);
+		//printk(KERN_INFO "pid=%d switched back to ATLAS\n", current->pid);
+	}
+	
+	/* 
+	 * the following code is identical to wait queue implementation
+	 * when waiting for a specific event.
+	 * we are aware of the "lost wakup" problem
+	 */
+	if (se->job) {
+		//reschedule task
+		set_tsk_need_resched(current);
+		goto out;
+	}
+	
+	se->state = ATLAS_BLOCKED;
+	
+	for(;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		
+		//we are aware of the lost update problem
+		if ((se->job = pop_task_job(se)))
+			break;
+		DEBUG(DEBUG_SYS_NEXT, "pid=%d no job, call schedule now", current->pid);
+
+		if (likely(!signal_pending(current))) {
+			schedule();
+			continue;
+		}
+	
+		/*
+		 * pending signal
+		 */
+		se->state = ATLAS_UNDEF;
+		__set_current_state(TASK_RUNNING);
+		ret = -EINTR;
+		goto out_no_timer;
+	}
+
+	__set_current_state(TASK_RUNNING);
+	se->state = ATLAS_RUNNING;
+
+	DEBUG(DEBUG_SYS_NEXT, "pid=%d job=%p job->d=%llu",
+		current->pid, se->job, ktime_to_us(se->job->deadline));
+	
+out:
+	/*
+	 * setup new timer
+	 * if the deadline has already passed, the callback will be called
+	 * resulting in a scheduler switch to CFS
+	 */
+	DEBUG(DEBUG_SYS_NEXT, "pid=%d setup timer for job %p (need_resched=%d).",
+		current->pid, se->job, test_tsk_need_resched(current));
+
+	hrtimer_start(&se->timer, se->job->deadline, HRTIMER_MODE_ABS_PINNED);
+
+out_no_timer:	
+	return ret;
+
+}
+
+
+#define ATLAS_TIME_ABS 0
+#define ATLAS_TIME_REL 1
+
+SYSCALL_DEFINE4(atlas_submit, pid_t, pid, struct timeval __user *,
+					exectime, struct timeval __user *, deadline, int, time_base)
+					
+{
+	struct timeval lexectime;
+	struct timeval ldeadline;
+	struct atlas_job *job;
+	struct task_struct *t;
+	int ret = 0;
+	ktime_t now, kdeadline;
+	struct atlas_rq *atlas_rq;
+	unsigned long flags;
+
+	//DEBUG(DEBUG_SYS_SUBMIT, "pid=%u, exectime=0x%p, deadline=0x%p",
+	//	pid, exectime, deadline);
+			
+	if (!exectime || !deadline || pid < 0)
+		return -EINVAL;
+					
+	if (copy_from_user(&lexectime, exectime, sizeof(struct timeval)) ||
+		copy_from_user(&ldeadline, deadline, sizeof(struct timeval))) {
+		DEBUG(DEBUG_SYS_SUBMIT, "bad address");
+		return -EFAULT;
+	}
+	DEBUG(DEBUG_SYS_SUBMIT, "pid=%u, exectime=%lld, deadline=%lld, time_base=%s",
+		pid,
+		ktime_to_ms(timeval_to_ktime(lexectime)),
+		ktime_to_ms(timeval_to_ktime(ldeadline)),
+		time_base == 0 ? "ABS" : ( time_base == 1 ? "REL" : "INVALID"));
+
+	/*
+	 * calculate deadline with respect to CLOCK_MONOTONIC
+	 */
+	kdeadline = timeval_to_ktime(ldeadline);
+	if (time_base == ATLAS_TIME_REL)
+		kdeadline = ktime_add(ktime_get(), kdeadline);
+
+	/*
+	 * allocate memory for the new job
+	 */
+	job = kmalloc(sizeof(struct atlas_job), 0);
+	DEBUG(DEBUG_SYS_SUBMIT, "job=%p", job);
+	if (job == NULL) {
+		return -ENOMEM;
+	}
+
+	rcu_read_lock();
+
+	/*
+	 * check for thread existence
+	 */
+	job->pid = find_get_pid(pid);
+	
+	if (!job->pid) {
+		kfree(job);
+		ret = -ESRCH;
+		goto out;
+	}
+	
+	t = pid_task(job->pid, PIDTYPE_PID);
+	BUG_ON(!t);
+	atlas_rq = &task_rq(t)->atlas;
+
+	init_job(job);
+	
+	job->deadline = kdeadline; 
+	job->exectime = timeval_to_ktime(lexectime);
+	
+	job->sdeadline = job->deadline;
+	job->sexectime = job->exectime;
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	//now = atlas_rq->exec_timer.base->get_time();
+	now = ktime_get();
+	
+	assign_rq_job(atlas_rq, job, ktime_get());
+
+	if (ktime_cmp(job->exectime, job->sexectime) == 0)
+		DEBUG(DEBUG_SYS_SUBMIT, "sexectime == exectime");
+	else if (ktime_zero(job->sexectime))
+		DEBUG(DEBUG_SYS_SUBMIT, "sexectime == 0");
+	else
+		DEBUG(DEBUG_SYS_SUBMIT, "sexectime < exectime");
+
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+
+	//rcu_read_lock prevents the job from going away
+	assign_task_job(t, job);
+	
+out:
+	DEBUG(DEBUG_SYS_SUBMIT, "ready: job=%p", job);
+	rcu_read_unlock();
+	put_job(job);
+	return ret;
+}
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 39c44fa..fa190a4 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -715,6 +715,7 @@ static void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 	update_rq_clock(rq);
 	sched_info_queued(p);
 	p->sched_class->enqueue_task(rq, p, flags);
+	trace_sched_enqueue_task(p, rq->cpu);
 }
 
 static void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
@@ -722,6 +723,7 @@ static void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 	update_rq_clock(rq);
 	sched_info_dequeued(p);
 	p->sched_class->dequeue_task(rq, p, flags);
+	trace_sched_dequeue_task(p, rq->cpu);
 }
 
 void activate_task(struct rq *rq, struct task_struct *p, int flags)
@@ -1061,12 +1063,17 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 	if (p->sched_class == rq->curr->sched_class) {
 		rq->curr->sched_class->check_preempt_curr(rq, p, flags);
 	} else {
-		for_each_class(class) {
-			if (class == rq->curr->sched_class)
-				break;
-			if (class == p->sched_class) {
-				resched_task(rq->curr);
-				break;
+		if (unlikely(rq->curr->sched_class == &atlas_sched_class) &&
+			unlikely(rq->curr->atlas.flags & ATLAS_FALLBACK))
+			resched_task(rq->curr);
+		else {
+			for_each_class(class) {
+				if (class == rq->curr->sched_class)
+					break;
+				if (class == p->sched_class) {
+					resched_task(rq->curr);
+					break;
+				}
 			}
 		}
 	}
@@ -1695,6 +1702,8 @@ int wake_up_state(struct task_struct *p, unsigned int state)
 	return try_to_wake_up(p, state, 0);
 }
 
+extern enum hrtimer_restart atlas_timer_task_function(struct hrtimer *timer);
+
 /*
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
@@ -1718,6 +1727,12 @@ static void __sched_fork(struct task_struct *p)
 #endif
 
 	INIT_LIST_HEAD(&p->rt.run_list);
+	
+	/*ATLAS stuff*/
+	INIT_LIST_HEAD(&p->atlas.jobs);
+	p->atlas.on_rq = 0;
+	hrtimer_init(&p->atlas.timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	p->atlas.timer.function = &atlas_timer_task_function;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
@@ -1749,7 +1764,7 @@ void sched_fork(struct task_struct *p)
 	 * Revert to default priority/policy on fork if requested.
 	 */
 	if (unlikely(p->sched_reset_on_fork)) {
-		if (task_has_rt_policy(p)) {
+		if (task_has_rt_policy(p) || unlikely(p->policy == SCHED_ATLAS)) {
 			p->policy = SCHED_NORMAL;
 			p->static_prio = NICE_TO_PRIO(0);
 			p->rt_priority = 0;
@@ -3353,16 +3368,20 @@ pick_next_task(struct rq *rq)
 	if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
 		p = fair_sched_class.pick_next_task(rq);
 		if (likely(p))
-			return p;
+			goto out;
 	}
 
 	for_each_class(class) {
 		p = class->pick_next_task(rq);
 		if (p)
-			return p;
+			goto out;
 	}
 
-	BUG(); /* the idle class will always have a runnable task */
+out:
+	BUG_ON(!p); /* the idle class will always have a runnable task */
+	
+	trace_sched_pick_next_task(p);
+	return p;
 }
 
 /*
@@ -3373,7 +3392,7 @@ static void __sched __schedule(void)
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
 	struct rq *rq;
-	int cpu;
+	int cpu, ret;
 
 need_resched:
 	preempt_disable();
@@ -3381,12 +3400,36 @@ need_resched:
 	rq = cpu_rq(cpu);
 	rcu_note_context_switch(cpu);
 	prev = rq->curr;
+	
+	/*
+	 * SCHED_ATLAS: if the prev has ATLAS_MISSED_DEADLINE flag, move it to CFS
+	 */
+	if (unlikely(prev->policy == SCHED_ATLAS) &&
+			unlikely(prev->atlas.flags & (ATLAS_MISSED_DEADLINE | ATLAS_EXHAUSTED_EXECTIME | ATLAS_OVERLOAD))) {
+		struct sched_param lparam = {0};
+		
+		/*
+		if (prev->atlas.flags & ATLAS_MISSED_DEADLINE)
+			printk_sched("schedule(): ATLAS_MISSED_DEADLINE\n");
+		if (prev->atlas.flags & ATLAS_EXHAUSTED_EXECTIME)
+			printk_sched("schedule(): ATLAS_EXHAUSTED_EXECTIME\n");
+		if (prev->atlas.flags & ATLAS_OVERLOAD)
+			printk_sched("schedule(): ATLAS_OVERLOAD\n");*/
+		ret = sched_setscheduler_nocheck(prev, SCHED_NORMAL, &lparam);
+		WARN_ON(ret);
+		//prev->atlas.flags &= ~ATLAS_MISSED_DEADLINE; //done in switched from
+		//clear_tsk_need_resched(prev); mhh? reschedule the task, because ATLAS has likely a more important one
+		sched_preempt_enable_no_resched();
+		//printk_sched("schedule(): switch scheduler to cfs of pid=%d\n", prev->pid);
+		//return;
+		goto need_resched;
+	}
 
 	schedule_debug(prev);
 
 	if (sched_feat(HRTICK))
 		hrtick_clear(rq);
-
+	
 	raw_spin_lock_irq(&rq->lock);
 
 	switch_count = &prev->nivcsw;
@@ -4223,6 +4266,8 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 	p->prio = rt_mutex_getprio(p);
 	if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	else if (policy == SCHED_ATLAS)
+		p->sched_class = &atlas_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 	set_load_weight(p);
@@ -4266,7 +4311,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy != SCHED_ATLAS)
 			return -EINVAL;
 	}
 
@@ -7267,6 +7312,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		init_atlas_rq(&rq->atlas);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c099cc6..38b5249d 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -615,6 +615,10 @@ static u64 __sched_period(unsigned long nr_running)
 	return period;
 }
 
+u64 cfs_sched_period(unsigned long nr_running) {
+	return __sched_period(nr_running);
+}
+
 /*
  * We calculate the wall-time slice from the period by taking a part
  * proportional to the weight.
@@ -5259,7 +5263,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &atlas_fallback_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 573e1ca..b272fa6 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2038,7 +2038,7 @@ static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)
 }
 
 const struct sched_class rt_sched_class = {
-	.next			= &fair_sched_class,
+	.next			= &atlas_sched_class,
 	.enqueue_task		= enqueue_task_rt,
 	.dequeue_task		= dequeue_task_rt,
 	.yield_task		= yield_task_rt,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5cf093f..928bf64 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -309,6 +309,40 @@ struct rt_rq {
 #endif
 };
 
+#define ATLAS_MISSED_DEADLINE    0x01
+#define ATLAS_OVERLOAD           0x02
+#define ATLAS_SYS_NEXT 		     0x04
+#define ATLAS_EXHAUSTED_EXECTIME 0x08
+#define ATLAS_BAD_TASK           0x10
+#define ATLAS_FALLBACK           0x20
+#define ATLAS_SLACK              0x40
+
+//needs to be defined here because of trace stuff
+struct atlas_job {
+	struct list_head list;
+	struct rb_node rb_node;
+	struct pid *pid;  //used to map submission -> map AND to distinguish task and gap
+	ktime_t exectime; //relative
+	ktime_t deadline; //absolut
+	ktime_t sdeadline;
+	ktime_t sexectime;
+	atomic_t count;
+};
+
+struct atlas_rq {
+	struct sched_atlas_entity *curr, *timer_se;
+	struct rb_root     tasks_timeline;
+	struct rb_node *rb_leftmost_se;
+	struct rb_root     jobs;
+	raw_spinlock_t			lock;
+	int nr_runnable;
+	struct hrtimer exec_timer;
+	struct hrtimer slack_timer;
+	struct hrtimer resched_timer;
+	ktime_t slack_end;
+	unsigned long flags;
+};
+
 #ifdef CONFIG_SMP
 
 /*
@@ -370,6 +404,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct atlas_rq atlas;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -847,7 +882,9 @@ enum cpuacct_stat_index {
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
+extern const struct sched_class atlas_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class atlas_fallback_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1143,6 +1180,7 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_atlas_rq(struct atlas_rq *atlas_rq);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 4ab1187..037c830 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -339,6 +339,20 @@ static struct ctl_table kern_table[] = {
 	},
 #endif
 	{
+		.procname	= "sched_atlas_min_slack",
+		.data		= &sysctl_sched_atlas_min_slack,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_atlas_slice",
+		.data		= &sysctl_sched_atlas_slice,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
 		.procname	= "sched_rt_period_us",
 		.data		= &sysctl_sched_rt_period,
 		.maxlen		= sizeof(unsigned int),
