diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index 7a35a6e..9916ed2 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -356,3 +356,6 @@
 347	i386	process_vm_readv	sys_process_vm_readv		compat_sys_process_vm_readv
 348	i386	process_vm_writev	sys_process_vm_writev		compat_sys_process_vm_writev
 349	i386	kcmp			sys_kcmp
+350	i386	atlas_next		sys_atlas_next
+351	i386	atlas_submit		sys_atlas_submit
+352	i386	atlas_debug		sys_atlas_debug
diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index a582bfe..43f9836 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -319,6 +319,9 @@
 310	64	process_vm_readv	sys_process_vm_readv
 311	64	process_vm_writev	sys_process_vm_writev
 312	common	kcmp			sys_kcmp
+313	common	atlas_next		sys_atlas_next
+314	64	atlas_submit		sys_atlas_submit
+315	common	atlas_debug		sys_atlas_debug
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/debian.quantal/config/amd64/config.flavour.atlas b/debian.quantal/config/amd64/config.flavour.atlas
new file mode 100644
index 0000000..06d8b53
--- /dev/null
+++ b/debian.quantal/config/amd64/config.flavour.atlas
@@ -0,0 +1,3 @@
+#
+# Config options for config.flavour.atlas automatically generated by splitconfig.pl
+#
diff --git a/debian.quantal/config/i386/config.flavour.atlas b/debian.quantal/config/i386/config.flavour.atlas
new file mode 100644
index 0000000..06d8b53
--- /dev/null
+++ b/debian.quantal/config/i386/config.flavour.atlas
@@ -0,0 +1,3 @@
+#
+# Config options for config.flavour.atlas automatically generated by splitconfig.pl
+#
diff --git a/debian.quantal/control.d/vars.atlas b/debian.quantal/control.d/vars.atlas
new file mode 100644
index 0000000..83e429e
--- /dev/null
+++ b/debian.quantal/control.d/vars.atlas
@@ -0,0 +1,6 @@
+arch="i386 amd64"
+supported="ATLAS"
+target="Experimental ATLAS scheduler."
+desc="=HUMAN= SMP"
+bootloader="grub-pc | grub-efi-amd64 | grub-efi-ia32 | grub | lilo (>= 19.1)"
+provides="kvm-api-4, redhat-cluster-modules, ivtv-modules, ndiswrapper-modules-1.9"
diff --git a/debian.quantal/rules.d/amd64.mk b/debian.quantal/rules.d/amd64.mk
index 0ef1186..8a98fc7 100644
--- a/debian.quantal/rules.d/amd64.mk
+++ b/debian.quantal/rules.d/amd64.mk
@@ -2,7 +2,7 @@ human_arch	= 64 bit x86
 build_arch	= x86_64
 header_arch	= $(build_arch)
 defconfig	= defconfig
-flavours	= generic
+flavours	= atlas
 build_image	= bzImage
 kernel_file	= arch/$(build_arch)/boot/bzImage
 install_file	= vmlinuz
diff --git a/debian.quantal/rules.d/i386.mk b/debian.quantal/rules.d/i386.mk
index 3e82c65..35feef3 100644
--- a/debian.quantal/rules.d/i386.mk
+++ b/debian.quantal/rules.d/i386.mk
@@ -2,7 +2,7 @@ human_arch	= 32 bit x86
 build_arch	= i386
 header_arch	= x86_64
 defconfig	= defconfig
-flavours        = generic
+flavours        = atlas
 build_image	= bzImage
 kernel_file	= arch/$(build_arch)/boot/bzImage
 install_file	= vmlinuz
diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9e65eff..025cff5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -159,6 +159,13 @@ extern struct cred init_cred;
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
 		.time_slice	= RR_TIMESLICE,				\
 	},								\
+	.atlas  = {						\
+		.state = ATLAS_UNDEF,		\
+		.flags = 0,					\
+		.submissions = LIST_HEAD_INIT(tsk.atlas.submissions),		\
+		.submission  = NULL,		\
+		.submissions_lock = __SPIN_LOCK_UNLOCKED(tsk.atlas.submissions_lock),	\
+	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	INIT_PUSHABLE_TASKS(tsk)					\
 	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a1f493..3f76aa8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,8 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+/* SCHED_ATLAS:  ATLAS Scheduler*/
+#define SCHED_ATLAS		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1211,6 +1213,28 @@ struct sched_rt_entity {
 #endif
 };
 
+enum atlas_state {
+		ATLAS_UNDEF,
+		ATLAS_BLOCKED,
+		ATLAS_RUNNING,
+};
+
+
+struct sched_atlas_entity {
+	struct rb_node     run_node; /*for normal operation*/
+	//struct list_head   run_list;  ??
+	struct list_head   list;     /*for initialization*/
+	unsigned int       state;
+	unsigned long      flags;
+	unsigned int       on_rq;
+	//struct atlas_rq    *atlas_rq; //needed?
+	
+	struct atlas_submission  *submission;
+	struct list_head         submissions;
+	spinlock_t               submissions_lock; // to lock list and recent submission
+	struct hrtimer 			 timer;
+};
+
 /*
  * default timeslice is 100 msecs (used only for SCHED_RR tasks).
  * Timeslices get refilled after they expire.
@@ -1244,6 +1268,7 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+	struct sched_atlas_entity atlas;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -2002,6 +2027,10 @@ extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 
+extern unsigned int sysctl_sched_atlas_min_slack;
+extern unsigned int sysctl_sched_atlas_slice;
+
+
 enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_NONE,
 	SCHED_TUNABLESCALING_LOG,
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 19439c7..15157f1 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -857,6 +857,11 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 				      const struct iovec __user *rvec,
 				      unsigned long riovcnt,
 				      unsigned long flags);
+asmlinkage long sys_atlas_next(void);
+asmlinkage long sys_atlas_submit(pid_t pid,
+					struct timeval __user *exectime,
+					struct timeval __user *deadline);
+asmlinkage long sys_atlas_debug(void);
 
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index ea7a203..5713c0d 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -150,6 +150,46 @@ TRACE_EVENT(sched_switch,
 );
 
 /*
+ * Tracepoint for pick_next_task.
+ */
+TRACE_EVENT(sched_pick_next_task,
+
+	TP_PROTO(struct task_struct *p),
+
+	TP_ARGS(p),
+
+	TP_STRUCT__entry(
+		__array(	char,	p_comm,	TASK_COMM_LEN	)
+		__field(	pid_t,	pid			)
+		__field(	int,	policy		)
+		__field(	unsigned long,	flags	)
+		__field(	int,	has_sub			)
+		__field(    void *, submission      )
+		__field(	s64,	sdeadline		)
+		__field(	s64,	deadline		)
+		__field(	s64,	sexectime		)
+		__field(	s64,	exectime		)
+		__field(	s64,	now				)
+	),
+
+	TP_fast_assign(
+		memcpy(__entry->p_comm, p->comm, TASK_COMM_LEN);
+		__entry->pid	 = p->pid;
+		__entry->policy  = p->policy;
+		__entry->flags   = p->atlas.flags;
+		__entry->submission= p->atlas.submission;
+		__entry->sdeadline = __entry->submission ? ktime_to_ns(p->atlas.submission->sdeadline) : 0;
+		__entry->deadline  = __entry->submission ? ktime_to_ns(p->atlas.submission->deadline) : 0;
+		__entry->sexectime = __entry->submission ? ktime_to_ns(p->atlas.submission->sexectime) : 0;
+		__entry->exectime  = __entry->submission ? ktime_to_ns(p->atlas.submission->exectime) : 0;
+		__entry->now       = ktime_to_ns(p->atlas.timer.base->get_time());
+	),
+
+	TP_printk("pid=%d",
+		__entry->pid)
+);
+
+/*
  * Tracepoint for a task being migrated:
  */
 TRACE_EVENT(sched_migrate_task,
diff --git a/kernel/exit.c b/kernel/exit.c
index 46ce8da..f438a6a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -60,6 +60,7 @@
 #include <asm/mmu_context.h>
 
 static void exit_mm(struct task_struct * tsk);
+extern void exit_atlas(struct task_struct * tsk);
 
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
@@ -991,6 +992,7 @@ void do_exit(long code)
 		acct_process();
 	trace_sched_process_exit(tsk);
 
+	exit_atlas(tsk);
 	exit_sem(tsk);
 	exit_shm(tsk);
 	exit_files(tsk);
diff --git a/kernel/printk.c b/kernel/printk.c
index 146827f..b827392 100644
--- a/kernel/printk.c
+++ b/kernel/printk.c
@@ -1875,21 +1875,33 @@ int is_console_locked(void)
 /*
  * Delayed printk version, for scheduler-internal messages:
  */
-#define PRINTK_BUF_SIZE		512
+#define PRINTK_BUF_SIZE		128
+#define PRINTK_BUF_NR		256
 
 #define PRINTK_PENDING_WAKEUP	0x01
 #define PRINTK_PENDING_SCHED	0x02
 
 static DEFINE_PER_CPU(int, printk_pending);
-static DEFINE_PER_CPU(char [PRINTK_BUF_SIZE], printk_sched_buf);
+
+typedef struct {
+	int idx;
+	char buf[PRINTK_BUF_NR][PRINTK_BUF_SIZE];
+} printk_sched_buf_t;
+
+static DEFINE_PER_CPU(printk_sched_buf_t, printk_sched_buf) = {.idx = 0 };
 
 void printk_tick(void)
 {
 	if (__this_cpu_read(printk_pending)) {
 		int pending = __this_cpu_xchg(printk_pending, 0);
 		if (pending & PRINTK_PENDING_SCHED) {
-			char *buf = __get_cpu_var(printk_sched_buf);
-			printk(KERN_WARNING "[sched_delayed] %s", buf);
+			printk_sched_buf_t *data = &__get_cpu_var(printk_sched_buf);
+			int i;
+			for (i = 0; i < data->idx; ++i) {
+				char *buf = data->buf[i]; 
+				printk(KERN_WARNING "[sched_delayed] %s", buf);
+			}
+			data->idx = 0;
 		}
 		if (pending & PRINTK_PENDING_WAKEUP)
 			wake_up_interruptible(&log_wait);
@@ -2362,16 +2374,19 @@ int printk_sched(const char *fmt, ...)
 {
 	unsigned long flags;
 	va_list args;
+	printk_sched_buf_t *data;
 	char *buf;
 	int r;
 
 	local_irq_save(flags);
-	buf = __get_cpu_var(printk_sched_buf);
-
+	data = &__get_cpu_var(printk_sched_buf);
+	BUG_ON(data->idx >= PRINTK_BUF_NR);
+	buf = data->buf[data->idx++];
+	
 	va_start(args, fmt);
 	r = vsnprintf(buf, PRINTK_BUF_SIZE, fmt, args);
 	va_end(args);
-
+	
 	__this_cpu_or(printk_pending, PRINTK_PENDING_SCHED);
 	local_irq_restore(flags);
 
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 173ea52..fc2ebc7 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o
+obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o atlas.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/atlas.c b/kernel/sched/atlas.c
new file mode 100644
index 0000000..f1415dd
--- /dev/null
+++ b/kernel/sched/atlas.c
@@ -0,0 +1,1758 @@
+#include <linux/syscalls.h>
+#include <linux/rbtree.h>
+#include <linux/slab.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include "sched.h"
+
+unsigned int sysctl_sched_atlas_min_slack = 1000000ULL;
+unsigned int sysctl_sched_atlas_slice    = 10000000ULL;
+
+const struct sched_class atlas_sched_class;
+
+extern u64 cfs_sched_period(unsigned long nr_running);
+
+
+#define SLACKTIME_CFS    0x1
+#define SLACKTIME_ATLAS  0x2
+#define SLACKTIME        0x3
+
+//#define ATLAS_DEBUG
+
+enum {
+		DEBUG_SYS_NEXT       = 1UL << 0,
+		DEBUG_SYS_SUBMIT     = 1UL << 1,
+		DEBUG_ENQUEUE        = 1UL << 2,
+		DEBUG_DEQUEUE        = 1UL << 3,
+		DEBUG_PICK_NEXT_TASK = 1UL << 4,
+		DEBUG_SET_CURR_TASK  = 1UL << 5,
+		DEBUG_SWITCHED_TO    = 1UL << 6,
+		DEBUG_PUT_PREV_TASK  = 1UL << 7,
+		DEBUG_CHECK_PREEMPT  = 1UL << 8,
+		DEBUG_RBTREE         = 1UL << 9,
+		DEBUG_TIMER          = 1UL << 10,
+		DEBUG_SUBMISSIONS    = 1UL << 11,
+		DEBUG_PICK_NEXT_TASK_FALLBACK = 1UL << 12,
+		DEBUG_ADAPT_SEXEC    = 1UL << 13,
+};
+
+#ifdef ATLAS_DEBUG
+//static const unsigned debug_mask = DEBUG_ENQUEUE | DEBUG_DEQUEUE | DEBUG_PUT_PREV_TASK |
+//			DEBUG_PICK_NEXT_TASK | DEBUG_SYS_NEXT | DEBUG_SYS_SUBMIT | DEBUG_SET_CURR_TASK |
+//			DEBUG_TIMER | DEBUG_CHECK_PREEMPT | DEBUG_PICK_NEXT_TASK_FALLBACK;
+//static const unsigned debug_mask = DEBUG_SUBMISSIONS | DEBUG_SYS_SUBMIT | DEBUG_ADAPT_SEXEC;
+//static const unsigned debug_mask = DEBUG_SYS_NEXT | DEBUG_CHECK_PREEMPT | DEBUG_PICK_NEXT_TASK |
+//	DEBUG_PUT_PREV_TASK | DEBUG_ENQUEUE | DEBUG_DEQUEUE | DEBUG_PICK_NEXT_TASK_FALLBACK;
+static const unsigned debug_mask = DEBUG_TIMER | DEBUG_PICK_NEXT_TASK | DEBUG_PICK_NEXT_TASK_FALLBACK
+	| DEBUG_PUT_PREV_TASK;
+
+static int printk_counter = 0;
+	#define DEBUG(T,STR,...) \
+		do { \
+			if (T & debug_mask)  { \
+			printk_sched("%d (%d): "#T ": " STR "\n", (printk_counter++), \
+				smp_processor_id(), ##__VA_ARGS__); \
+			} \
+		} while(0)
+	
+	#define DEBUG_ON(T) if (debug_mask & (T))
+			
+#else 
+	#define DEBUG(...)
+	#define DEBUG_ON(T) if (0)
+#endif /* ATLAS_DEBUG */
+
+
+#define collision(a,b) \
+	({struct timeval _a = ktime_to_timeval(a->sdeadline), \
+		_b = ktime_to_timeval(ktime_sub(b->sdeadline, b->sexectime)); \
+	  (timeval_compare(&_a, &_b) > 0 ? 1 : 0); })
+
+
+static inline struct atlas_submission *get_submission
+	(struct atlas_submission  *submission)
+{
+	if (submission)
+		atomic_inc(&submission->count);
+	return submission;
+}
+
+
+static void put_submission(struct atlas_submission *submission)
+{
+	if (!submission)
+		return;
+
+	if (atomic_dec_and_test(&submission->count)) {
+		//printk("free submission=%p\n", submission);
+		put_pid(submission->pid);
+		kfree(submission);
+	}
+}
+
+static inline int submission_before(struct atlas_submission *a,
+		struct atlas_submission *b)
+{
+	return ktime_to_ns(a->deadline) <  ktime_to_ns(b->deadline);
+}
+
+static int entity_before(struct sched_atlas_entity *a,
+		struct sched_atlas_entity *b)
+{
+	
+	/*
+	 * a preemption within sys_next or a wakeup due to a signal can lead
+	 * into cases where se->submission is null.
+	 * Because we also queue this se's into the tree, we have to check
+	 * both.
+	 * 
+	 * 4 cases:
+	 * new | comparator
+	 * ----------------
+	 *  o  |  o  doesn't matter
+	 *  o  |  x  new should go to the beginning
+	 *  x  |  o  the old entry should stay on the left side
+	 *  x  |  x  compare
+	 */
+	 
+	if (unlikely(!a->submission)) //left side if new has no submisson
+		return 1;
+	
+	if (unlikely(!b->submission)) //right side
+		return 0;
+		
+	return submission_before(a->submission, b->submission);
+}
+
+static void enqueue_entity(struct atlas_rq *atlas_rq,
+		struct sched_atlas_entity *se)
+{
+	struct rb_node **link = &atlas_rq->tasks_timeline.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_atlas_entity *entry;
+	int leftmost = 1;
+	
+	//rb_init_node(&se->run_node);
+	
+	DEBUG(DEBUG_RBTREE, "enqueue_task_rb_tree");
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_atlas_entity, run_node);
+		
+		if (entity_before(se, entry))
+			link = &parent->rb_left;
+		else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		atlas_rq->rb_leftmost_se = &se->run_node;
+	
+	rb_link_node(&se->run_node, parent, link);
+	rb_insert_color(&se->run_node, &atlas_rq->tasks_timeline);	
+}
+
+static void dequeue_entity(struct atlas_rq *atlas_rq,
+		struct sched_atlas_entity *se)
+{
+	DEBUG(DEBUG_RBTREE, "dequeue_task_rb_tree");
+
+	if (atlas_rq->rb_leftmost_se == &se->run_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&se->run_node);
+		atlas_rq->rb_leftmost_se = next_node;
+	}
+	
+	rb_erase(&se->run_node, &atlas_rq->tasks_timeline);
+}
+
+static struct sched_atlas_entity *pick_first_entity(struct atlas_rq *atlas_rq)
+{
+	struct rb_node *left = atlas_rq->rb_leftmost_se;
+
+	if (!left)
+		return NULL;
+
+	return rb_entry(left, struct sched_atlas_entity, run_node);
+}
+
+static struct sched_atlas_entity *pick_next_entity(struct sched_atlas_entity *se)
+{
+	struct rb_node *next = rb_next(&se->run_node);
+
+	if (!next)
+		return NULL;
+
+	return rb_entry(next, struct sched_atlas_entity, run_node);
+}
+
+static struct atlas_submission *pick_first_submission(struct atlas_rq *atlas_rq) {
+	struct rb_node *first = rb_first(&atlas_rq->submissions);
+
+	if (!first)
+		return NULL;
+	
+	return rb_entry(first, struct atlas_submission, rb_node);
+}
+
+static struct atlas_submission *pick_last_submission(struct atlas_rq *atlas_rq) {
+	struct rb_node *last = rb_last(&atlas_rq->submissions);
+
+	if (!last)
+		return NULL;
+	
+	return rb_entry(last, struct atlas_submission, rb_node);
+}
+
+static struct atlas_submission *pick_next_submission(struct atlas_submission *s) {
+	struct rb_node *next = rb_next(&s->rb_node);
+	
+	if (!next)
+		return NULL;
+	
+	return rb_entry(next, struct atlas_submission, rb_node);
+}
+
+static struct atlas_submission *pick_prev_submission(struct atlas_submission *s) {
+	struct rb_node *prev = rb_prev(&s->rb_node);
+	
+	if (!prev)
+		return NULL;
+	
+	return rb_entry(prev, struct atlas_submission, rb_node);
+}
+
+static inline int submission_in_rq(struct atlas_submission *s) {
+	return !RB_EMPTY_NODE(&s->rb_node);
+}
+
+static inline struct task_struct *task_of_submission(struct atlas_submission *s) {
+	return get_pid_task(s->pid, PIDTYPE_PID);
+}
+
+static inline int in_slacktime(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME);
+}
+
+static inline int in_slacktime_cfs(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME_CFS);
+}
+
+static inline int in_slacktime_atlas(struct atlas_rq *atlas_rq) {
+	return (atlas_rq->flags & SLACKTIME_ATLAS);
+}
+
+static inline ktime_t ktime_min(ktime_t a, ktime_t b) {
+	return ns_to_ktime(min(ktime_to_ns(a), ktime_to_ns(b)));
+}
+
+static inline int ktime_neg(ktime_t a) {
+	return ktime_to_ns(a) < 0;
+}
+
+static inline int ktime_zero(ktime_t a) {
+	return ktime_equal(ktime_set(0,0), a);
+}
+
+
+static ktime_t get_submission_start(struct atlas_submission *s) {
+	return ktime_sub(s->sdeadline, s->sexectime);
+}
+
+static inline int submission_missed_deadline(struct atlas_submission *s, ktime_t now) {
+	return ktime_neg(ktime_sub(s->sdeadline, now));
+}
+
+static inline int ktime_cmp(ktime_t a, ktime_t b) {
+	ktime_t tmp = ktime_sub(a, b);
+	if (ktime_to_ns(tmp) > 0)
+		return 1;
+	else if (ktime_equal(ktime_set(0,0), tmp))
+		return 0;
+	return -1;
+}
+
+static inline struct rq *rq_of(struct atlas_rq *atlas_rq)
+{
+	return container_of(atlas_rq, struct rq, atlas);
+}
+
+static inline struct task_struct *task_of(struct sched_atlas_entity *se)
+{
+	return container_of(se, struct task_struct, atlas);
+}
+
+static void start_new_slack(struct atlas_rq *atlas_rq, ktime_t slack) {
+	ktime_t now, tmp, cfs_slice;
+	
+	now = atlas_rq->slack_timer.base->get_time();
+	atlas_rq->slack_end = ktime_add(now, slack);
+	
+	cfs_slice = ns_to_ktime(cfs_sched_period(rq_of(atlas_rq)->cfs.nr_running));
+	
+	tmp = ktime_min(cfs_slice, slack);
+	DEBUG(DEBUG_TIMER, "Slacktime up to: %lld",
+			ktime_to_us(atlas_rq->slack_end));
+
+	hrtimer_start(&atlas_rq->slack_timer, tmp, HRTIMER_MODE_REL_PINNED);
+	atlas_rq->flags |= SLACKTIME_CFS;
+}
+
+static void cancel_slack(struct atlas_rq *atlas_rq) {
+	//slack time makes no sense if there are no runnable tasks
+	
+	if (!(atlas_rq->flags & SLACKTIME))
+		return;
+
+	if (!atlas_rq->nr_runnable) {
+		hrtimer_cancel(&atlas_rq->slack_timer);
+		DEBUG(DEBUG_TIMER, "reset slack timer, no runnable tasks.");
+		atlas_rq->flags &= ~SLACKTIME;
+	}
+}
+
+#ifdef ATLAS_DEBUG
+
+static struct sched_atlas_entity *pick_first_entity(struct atlas_rq *atlas_rq);
+static struct sched_atlas_entity *pick_next_entity(struct sched_atlas_entity *se);
+/*
+ * rq must be locked
+ */
+static void __debug_rq(struct rq *rq) {
+	struct sched_atlas_entity *se;
+	
+	printk_sched("SCHED_ATLAS: DEBUG rq=%d\n", cpu_of(rq));
+	printk_sched("    Currently running: %d\n", rq->atlas.nr_runnable);
+	printk_sched("    Curr: pid=%d\n", rq->atlas.curr ? task_of(rq->atlas.curr)->pid : -1);
+	
+	printk_sched("    DEBUG tasks_timeline:\n");
+	se = pick_first_entity(&rq->atlas);
+	while (se) {
+		printk_sched("        pid=%5d, submission=%p", task_of(se)->pid, se->submission);
+		se = pick_next_entity(se);	
+	}
+	printk_sched(KERN_INFO "======================\n");
+}
+
+static void debug_rq(struct rq *rq) {
+	unsigned long flags;
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	__debug_rq(rq);
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+static void debug_task(struct task_struct *p) {
+	unsigned counter = 0;
+	struct atlas_submission *submission;
+	struct sched_atlas_entity *se = &p->atlas;
+	const char *s;
+	
+	printk(KERN_INFO "SCHED_ATLAS: DEBUG task pid=%d", p->pid);
+	switch (p->atlas.state) {
+	case ATLAS_BLOCKED:
+		s = "ATLAS_BLOCKED";
+		break;
+	case ATLAS_UNDEF:
+		s = "ATLAS_UNDEF";
+		break;
+	case ATLAS_RUNNING:
+		s = "ATLAS_RUNNING";
+		break;
+	default:
+		s = "UNKNOWN";
+	}
+	
+	printk(KERN_INFO "State: %s\n", s);
+	printk(KERN_INFO "Submissions:\n");
+	spin_lock(&p->atlas.submissions_lock);
+	printk(KERN_INFO "se->submission=%p\n", p->atlas.submission);
+	list_for_each_entry(submission, &se->submissions, list) {
+		counter++;
+		printk(KERN_INFO "    %p\n", submission);
+	}
+	printk(KERN_INFO "    count: %d", counter);
+	printk(KERN_INFO "======================\n");
+	spin_unlock(&p->atlas.submissions_lock);
+}
+
+static void debug_submission(struct atlas_submission *s) {
+	//printk_sched("DEBUG_SUBMISSIONS: d=%6lld sd=%6lld e=%6lld se=%6lld (%p)\n",
+	//	ktime_to_ms(s->deadline),
+	//	ktime_to_ms(s->sdeadline),
+	//	ktime_to_ms(s->exectime),
+	//	ktime_to_ms(s->sexectime),
+	//	s);
+	printk("DEBUG_SUBMISSIONS: %6lld - %6lld (%6lld - %6lld) (%p)\n",
+		ktime_to_ms(ktime_sub(s->sdeadline, s->sexectime)),
+		ktime_to_ms(s->sdeadline),
+		ktime_to_ms(ktime_sub(s->deadline, s->exectime)),
+		ktime_to_ms(s->deadline),
+		s);
+}
+
+static void debug_submissions(struct atlas_rq *atlas_rq) {
+	struct atlas_submission *submission, *prev = NULL;
+
+	assert_raw_spin_locked(&atlas_rq->lock);
+	submission = pick_first_submission(atlas_rq);
+	printk("DEBUG_SUBMISSIONS:\n");
+	while (submission) {
+		if (prev) {
+			ktime_t start, end, diff;
+			start = prev->sdeadline;
+			end = get_submission_start(submission);
+			diff = ktime_sub(end, start);
+			if (!ktime_zero(diff)) {
+				printk("DEBUG_SUBMISSIONS: %6lld - %6lld (gap=%lld)\n",
+				ktime_to_ms(start),
+				ktime_to_ms(end),
+				ktime_to_ms(diff));
+			}
+		}
+		debug_submission(submission);
+		prev = submission;
+		submission = pick_next_submission(submission);
+	}
+}
+#endif /* ATLAS_DEBUG */
+
+
+
+
+/*
+ * must be called with lock hold
+ */
+static void push_task_submission(struct sched_atlas_entity *se,
+		struct atlas_submission *new_submission)
+{
+	struct list_head *entry;
+	struct atlas_submission *submission;
+
+	assert_spin_locked(&se->submissions_lock);
+	
+	//typically, a new submission should go to the end
+	list_for_each_prev(entry, &se->submissions) {
+		submission = list_entry(entry, struct atlas_submission, list);
+		if (submission_before(submission, new_submission))
+			goto out;
+	}
+out:
+	list_add(&new_submission->list, entry);
+}
+
+static struct atlas_submission *pop_task_submission(struct sched_atlas_entity *se)
+{
+	struct atlas_submission *s = NULL;
+	struct list_head *elem;
+	
+	spin_lock(&se->submissions_lock);
+	
+	if (list_empty(&se->submissions))
+		goto out;
+	
+	elem = se->submissions.next;
+	s = list_entry(elem, struct atlas_submission, list);
+	list_del(elem);
+out:
+	spin_unlock(&se->submissions_lock);
+	return s;
+}
+
+/* 
+ * must be called with rcu_read_lock hold
+ */
+static void assign_task_submission(struct atlas_submission *submission)
+{
+	struct task_struct *p;
+	struct sched_atlas_entity *se;
+	unsigned wakeup = 0;
+	
+	
+	p = pid_task(submission->pid, PIDTYPE_PID);
+	
+	if (!p)
+		return;
+	
+	//we need this submission later on
+	get_submission(submission);
+	
+	se = &p->atlas;
+
+	spin_lock(&se->submissions_lock);
+	wakeup = list_empty(&se->submissions);
+	push_task_submission(se, submission);
+	spin_unlock(&se->submissions_lock);
+	
+	/*
+	 * wake up process (we might wake up a process who does not wait for
+	 * the next submission, but who cares...) 
+	 */
+	if (wakeup)
+		wake_up_process(p);
+}
+
+
+/*******************************************************
+ * Scheduler stuff
+ */
+ 
+enum hrtimer_restart timer_rq_exec_func(struct hrtimer *timer);
+enum hrtimer_restart timer_rq_slack_func(struct hrtimer *timer);
+ 
+void init_atlas_rq(struct atlas_rq *atlas_rq)
+{
+	atlas_rq->curr = NULL;
+    atlas_rq->tasks_timeline = RB_ROOT;
+	atlas_rq->rb_leftmost_se = NULL;
+    atlas_rq->nr_runnable = 0;
+    //printk(KERN_INFO "INIT_ATLAS_RUNQUEUE(%d): %p",
+	//	cpu_of(rq_of(atlas_rq)), atlas_rq);
+	atlas_rq->submissions = RB_ROOT;
+	hrtimer_init(&atlas_rq->exec_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	atlas_rq->exec_timer.function = &timer_rq_exec_func;
+	hrtimer_init(&atlas_rq->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	atlas_rq->slack_timer.function = &timer_rq_slack_func;
+	atlas_rq->flags = 0;
+}
+
+/*
+ * We are picking a new current task - update its stats:
+ */
+static inline void
+update_stats_curr_start(struct atlas_rq *atlas_rq, struct sched_atlas_entity *se)
+{
+	/*
+	 * We are starting a new run period:
+	 */
+	task_of(se)->se.exec_start = rq_of(atlas_rq)->clock_task;
+}
+
+static int update_execution_time(struct atlas_rq *atlas_rq,
+	struct atlas_submission *submission, ktime_t delta_exec);
+
+static void update_curr_atlas(struct rq *rq)
+{
+    //copied from rt
+	struct task_struct *curr = rq->curr;
+	struct sched_atlas_entity *se = &curr->atlas;
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	u64 delta_exec;
+	struct atlas_submission *submission = se->submission;
+	unsigned long flags;
+
+	if (curr->sched_class != &atlas_sched_class)
+		return;
+
+	delta_exec = rq->clock_task - curr->se.exec_start;
+	if (unlikely((s64)delta_exec < 0))
+		delta_exec = 0;
+
+	schedstat_set(curr->se.statistics.exec_max, max(curr->se.statistics.exec_max, delta_exec));
+
+	curr->se.sum_exec_runtime += delta_exec;
+	account_group_exec_runtime(curr, delta_exec);
+
+	update_stats_curr_start(atlas_rq, se);
+	cpuacct_charge(curr, delta_exec);
+	
+	//we are going to change the submission tree, so lock it
+	//UPDTAE: if we are called because of a scheduling tick we are in interrupt
+	//context and have to skip the update because the lock might not be free!!
+
+	/*
+	 * when kworker is woken up, we are in interrupt context,
+	 * but we want to update the plan of execution
+	 */
+	//if (in_interrupt())
+	//	return;
+
+	if (unlikely(!submission))
+		return;
+		
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	update_execution_time(atlas_rq, submission, ns_to_ktime(delta_exec)); 
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+}
+
+
+
+/*
+ * the new task may be enqueued with the ATLAS_MISSED_DEADLINE bit set
+ * 
+ * -> in this case, the resched bit should has been set by the timer routine
+ * 
+ */
+static void enqueue_task_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &p->atlas;
+	
+	WARN_ON((se->flags & ATLAS_MISSED_DEADLINE) && !(test_tsk_need_resched(p)));
+	
+	DEBUG(DEBUG_ENQUEUE, "p->pid=%d curr->pid=%d atlas->curr->pid=%d", p->pid,
+		rq->curr ? rq->curr->pid : -1,
+		rq->atlas.curr ? task_of(rq->atlas.curr)->pid : -1);
+	
+	if (atlas_rq->curr != se)
+		enqueue_entity(atlas_rq, se);
+    
+    //mark task as on runqueue now
+	se->on_rq = 1;
+    atlas_rq->nr_runnable++;
+    
+    inc_nr_running(rq);
+	return;
+}
+
+
+/*
+ * also called when switching to CFS because of missed deadline
+ */
+static void dequeue_task_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &p->atlas;
+	
+	DEBUG(DEBUG_DEQUEUE, "pid=%d", p ? p->pid : -1);
+	
+	update_curr_atlas(rq);	
+
+    if (atlas_rq->curr == se)
+		atlas_rq->curr = NULL;
+	else
+		dequeue_entity(atlas_rq, se);
+	
+	se->on_rq = 0;
+	
+    atlas_rq->nr_runnable--;
+    
+    dec_nr_running(rq);
+	return;
+}
+
+static void yield_task_atlas(struct rq *rq)
+{
+    return;
+}
+
+static void check_preempt_curr_atlas(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_atlas_entity *se = &curr->atlas, *pse = &p->atlas;
+	int sub = (se->submission != NULL), psub = (pse->submission != NULL);
+	
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d", p->pid);
+	
+	if (unlikely(se == pse)) {
+		DEBUG(DEBUG_CHECK_PREEMPT, "se == pse; pid=%d don't preempt curr->pid=%d",
+			p->pid, curr->pid);
+		return;
+	}
+	
+	if (test_tsk_need_resched(curr)) {
+		DEBUG(DEBUG_CHECK_PREEMPT, "test_tsk_need_resched; pid=%d don't preempt curr->pid=%d",
+			p->pid, curr->pid);
+		return;
+	}
+	
+		
+	/* Idle tasks are by definition preempted by non-idle tasks. */
+	if (unlikely(curr->policy == SCHED_IDLE) &&
+	    likely(p->policy != SCHED_IDLE))
+		goto preempt;
+	
+	/* Preempt tasks scheduled by cfs */
+	if (likely(curr->policy == SCHED_NORMAL))
+		goto preempt;
+	
+	/* Bug if task is not scheduled by us */
+	BUG_ON(curr->policy != SCHED_ATLAS);
+	
+	/* if the new task has no submission, preempt */
+	if (unlikely(!psub))
+		goto preempt;
+	
+	/* if the currently running task has no submission, don't preempt */
+	if (unlikely(!sub))
+		goto no_preempt;
+		
+	if (submission_before(pse->submission, se->submission))
+		goto preempt;
+	
+no_preempt:
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d don't preempt curr->pid=%d",
+		p->pid, curr->pid);
+
+	return;
+	
+preempt:
+	DEBUG(DEBUG_CHECK_PREEMPT, "pid=%d preempt curr->pid=%d",
+		p->pid, curr->pid);
+	resched_task(curr);
+
+	return;
+}
+
+static int get_slacktime(struct atlas_rq *atlas_rq, ktime_t *slack);
+static void cleanup_rq(struct atlas_rq *atlas_rq);
+static inline void setup_rq_exec_timer(struct atlas_rq *atlas_rq,
+		struct atlas_submission *s);
+
+static struct task_struct *pick_next_task_atlas(struct rq *rq)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se;
+	ktime_t slack;
+	struct atlas_submission *s, *s_next;
+	struct task_struct *p;
+	unsigned long flags;
+
+	/*
+	 * only proceed if there are runnable tasks
+	 */
+	if (likely(!atlas_rq->nr_runnable)) {
+		//since there is no ready task, no need to set up timer
+		return NULL;
+	}
+
+	/*
+	 * if there is a currently running task a previous work was interrupted
+	 */
+	if (likely(atlas_rq->curr != NULL)) {
+		goto out;
+	}
+	
+	/*
+	 * slack time reserved for CFS?
+	 */
+	if (in_slacktime_cfs(atlas_rq))
+		return NULL;
+	
+	
+	se = pick_first_entity(atlas_rq);
+
+	/*
+	 * jobs without a submission are doing signal handling
+	 */
+	if (unlikely(!se->submission)) {
+		atlas_rq->curr = se;
+		dequeue_entity(atlas_rq, se);
+		goto out;
+	}
+
+	/*
+	 * slacktime for ATLAS?
+	 */
+	if (in_slacktime_atlas(atlas_rq)) {
+		se->flags |= ATLAS_SLACK; 
+		atlas_rq->curr = se;
+		dequeue_entity(atlas_rq, se);
+		goto out;
+	}
+
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	
+	/*
+	 * remove submissions having a deadline in the past
+	 */
+	cleanup_rq(atlas_rq);
+	
+	/*
+	 * submission of se might be already removed from the rq because of cleanup
+	 * -> move the task to CFS (it is rescheduled again, so we are called again)
+	 */
+	if (unlikely(!submission_in_rq(se->submission))) {
+		se->flags |= ATLAS_MISSED_DEADLINE;
+		resched_task(task_of(se));
+		atlas_rq->curr = se;
+		goto unlock_out;
+	}
+
+	//check if there is slack time left
+	if (get_slacktime(atlas_rq, &slack))
+	{
+		start_new_slack(atlas_rq, slack);
+		//start new slack time, return NULL, because CFS will start anyway
+		goto unlock_out;
+	}
+
+	//no slack time
+	s = se->submission;
+	s_next = pick_first_submission(atlas_rq);
+	while (s != s_next) {
+		if (!ktime_zero(s_next->sexectime)) {
+			p = task_of_submission(s_next);
+			if (p && p->policy != SCHED_ATLAS) {
+				//setup timer, because it is not done below
+				//since atlas->current == NULL !!
+				setup_rq_exec_timer(atlas_rq, s_next);
+				goto unlock_out;
+			}
+		}
+		s_next = pick_next_submission(s_next);
+	}
+
+	atlas_rq->curr = se;
+	dequeue_entity(atlas_rq, se);
+
+unlock_out:
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+
+out:	
+	if (atlas_rq->curr) {
+		DEBUG(DEBUG_PICK_NEXT_TASK, "pid=%d", task_of(atlas_rq->curr)->pid);
+		update_stats_curr_start(atlas_rq, atlas_rq->curr);
+		setup_rq_exec_timer(atlas_rq, atlas_rq->curr->submission);
+    	return task_of(atlas_rq->curr);
+	} else {
+		DEBUG(DEBUG_PICK_NEXT_TASK, "NULL");
+		return NULL;
+	}
+}
+
+static struct task_struct *pick_next_task_atlas_fallback(struct rq *rq)
+{
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se;
+	
+	if (likely(!atlas_rq->nr_runnable)) {
+		return NULL;
+	}
+
+	BUG_ON(atlas_rq->curr);
+	//if there are running tasks, we are processing in slack_time
+	BUG_ON(!in_slacktime(atlas_rq));
+	
+	se = pick_first_entity(atlas_rq);
+	atlas_rq->curr = se;
+	dequeue_entity(atlas_rq, se);
+	
+	update_stats_curr_start(atlas_rq, se);
+
+	//mark as fallback decision, needed by check_preempt_current
+	se->flags |= ATLAS_FALLBACK;
+	
+	BUG_ON(!se->submission);
+	hrtimer_start(&atlas_rq->exec_timer, se->submission->sexectime,
+		HRTIMER_MODE_REL_PINNED);	
+
+	DEBUG(DEBUG_PICK_NEXT_TASK_FALLBACK, "pid=%d", task_of(se)->pid);
+	return task_of(se);
+}
+
+static void put_prev_task_atlas(struct rq *rq, struct task_struct *prev)
+{	
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	struct sched_atlas_entity *se = &prev->atlas;
+	
+	DEBUG(DEBUG_PUT_PREV_TASK, "pid=%d (on_rq=%d, missed_deadline=%lu, fallback=%lu)", prev->pid,
+		se->on_rq, se->flags & ATLAS_MISSED_DEADLINE,
+		se->flags & ATLAS_FALLBACK);
+	
+	hrtimer_cancel(&rq->atlas.exec_timer);
+
+	se->flags &= ~(ATLAS_FALLBACK | ATLAS_SLACK);
+
+	rq->atlas.curr = NULL;
+	
+	if (se->on_rq) {
+		update_curr_atlas(rq);
+		enqueue_entity(atlas_rq, se);
+	}
+
+	cancel_slack(atlas_rq);
+	
+	return;
+}
+
+static void set_curr_task_atlas(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+	struct sched_atlas_entity *se = &p->atlas;
+	struct atlas_rq *atlas_rq = &rq->atlas;
+	
+	DEBUG(DEBUG_SET_CURR_TASK, "pid=%d", p->pid);
+    update_stats_curr_start(atlas_rq, se); 
+    
+    WARN_ON(rq->atlas.curr);
+	rq->atlas.curr = se;
+    
+    return;
+}
+
+static void task_tick_atlas(struct rq *rq, struct task_struct *p, int queued)
+{
+    update_curr_atlas(rq);
+    return;
+}
+
+static void prio_changed_atlas(struct rq *rq, struct task_struct *p, int oldprio)
+{
+    //printk(KERN_INFO "SCHED_ATLAS: prio_changed\n");
+    return;
+}
+
+static void switched_from_atlas(struct rq *rq, struct task_struct *p)
+{
+    //printk(KERN_INFO "SCHED_ATLAS: switched_from\n");
+    return;
+}
+
+static void switched_to_atlas(struct rq *rq, struct task_struct *p)
+{
+    DEBUG(DEBUG_SWITCHED_TO, "pid=%d", p->pid);
+    return;
+}   
+
+static unsigned int get_rr_interval_atlas(struct rq *rq, struct task_struct *task)
+{
+    printk(KERN_INFO "SCHED_ATLAS: get_rr_interval\n");
+    return 0;
+}
+
+#ifdef CONFIG_SMP
+static int select_task_rq_atlas(struct task_struct *p, int sd_flag, int flags)
+{
+    return task_cpu(p);
+    
+}
+#endif /* CONFIG_SMP */
+ 
+ 
+ 
+/**
+ * timer related stuff
+ */
+
+/*
+ * called when a process missed its deadline; called from irq context
+ */
+enum hrtimer_restart atlas_timer_task_function(struct hrtimer *timer)
+{
+	struct sched_atlas_entity *se = container_of(timer, struct sched_atlas_entity, timer);
+	struct task_struct *p = task_of(se);
+
+	/*
+	 * optimization: if se->submission == NULL the task is in between sys_next
+	 * and there is no need to set the flag.
+	 */
+	
+	WARN_ON(!se->submission);
+	se->flags |= ATLAS_MISSED_DEADLINE;
+	DEBUG(DEBUG_TIMER, "deadline missed: pid=%d", task_of(se)->pid);
+	wmb();
+	set_tsk_need_resched(p);
+	
+	return HRTIMER_NORESTART;
+}
+
+enum hrtimer_restart timer_rq_exec_func(struct hrtimer *timer)
+{
+	struct atlas_rq *atlas_rq = container_of(timer, struct atlas_rq, exec_timer);
+	struct rq *rq = rq_of(atlas_rq);
+	unsigned long flags;
+	
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (rq->curr) {
+		if (rq->curr == task_of(atlas_rq->curr))
+			atlas_rq->curr->flags |= ATLAS_EXHAUSTED_EXECTIME;
+		resched_task(rq->curr);
+	}
+	DEBUG(DEBUG_TIMER, "exec_func, resched pid=%d", rq->curr ? rq->curr->pid : -1);
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	return HRTIMER_NORESTART;
+}
+
+static inline void setup_rq_exec_timer(struct atlas_rq *atlas_rq,
+		struct atlas_submission *s) {
+
+	if (unlikely(!s))
+		return;
+
+	BUG_ON(ktime_zero(s->sexectime) || ktime_neg(s->sexectime));
+	hrtimer_start(&atlas_rq->exec_timer, s->sexectime, HRTIMER_MODE_REL_PINNED);
+}
+
+/*
+ * handle slack time transitions
+ */
+enum hrtimer_restart timer_rq_slack_func(struct hrtimer *timer)
+{
+	struct atlas_rq *atlas_rq = container_of(timer, struct atlas_rq, slack_timer);
+	struct rq *rq = rq_of(atlas_rq);
+	unsigned long flags;
+	ktime_t now, remaining, slack;
+	
+	now = timer->base->get_time();
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+
+	BUG_ON(!(atlas_rq->flags & SLACKTIME));
+	
+	remaining = ktime_sub(atlas_rq->slack_end, now);
+	if (ktime_cmp(remaining, ns_to_ktime(sysctl_sched_atlas_min_slack)) <= 0) {
+		//no more slack time left
+		atlas_rq->flags &= ~SLACKTIME;
+		DEBUG(DEBUG_TIMER, "slack_func: no more slack time left");
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+		return HRTIMER_NORESTART;
+	}
+
+	if (atlas_rq->flags & SLACKTIME_CFS) {
+		ktime_t atlas_slice = ns_to_ktime(sysctl_sched_atlas_slice);
+		//it's time for atlas
+		atlas_rq->flags &= ~SLACKTIME;
+		atlas_rq->flags |= SLACKTIME_ATLAS;
+		
+		slack = ktime_min(remaining, atlas_slice);
+	} else {
+		ktime_t cfs_slice = ns_to_ktime(cfs_sched_period(rq->cfs.nr_running));
+		//time for cfs
+		atlas_rq->flags &= ~SLACKTIME;
+		atlas_rq->flags |= SLACKTIME_CFS;
+	
+		slack = ktime_min(remaining, cfs_slice);
+	}
+	hrtimer_forward_now(timer, slack);
+	//in case timer was programmed for slack time
+
+	if (rq->curr)
+		resched_task(rq->curr);
+
+	DEBUG(DEBUG_TIMER, "slack_func: switch to %s, resched pid=%d, slack=%llu",
+		atlas_rq->flags & SLACKTIME_CFS ? "CFS" : "ATLAS",
+		rq->curr ? rq->curr->pid : -1,
+		ktime_to_ms(slack));
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	return HRTIMER_RESTART;
+}
+/*
+ * Methods to maintain submission tree.
+ */
+
+/*
+ * find the first submission after a given submission
+ * returns NULL if there is no such submission
+ */
+static struct atlas_submission *find_submission_pos(struct atlas_rq *atlas_rq,
+		struct atlas_submission *submission)
+{	
+	struct rb_node **link;
+	struct rb_node *parent = NULL;
+	struct atlas_submission *entry;
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	link = &atlas_rq->submissions.rb_node;
+	
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct atlas_submission, rb_node);
+		
+		if (submission_before(submission, entry))
+			link = &parent->rb_left;
+		else
+			link = &parent->rb_right;
+	}
+	
+	if (!parent)
+		return NULL;
+	
+	if (link == &parent->rb_right)
+		parent = rb_next(parent);
+	
+	if (!parent)
+		return NULL;
+	
+	
+	entry = rb_entry(parent, struct atlas_submission, rb_node);
+	//entry is the submission following our submission
+	return entry;
+}
+ 
+static inline void resolve_collision(struct atlas_submission *a,
+		struct atlas_submission *b) {
+	a->sdeadline = ktime_sub(b->sdeadline, b->sexectime);
+}
+
+/*
+ * close the gap between submission a and b and
+ * return 1 iff start of submission a was moved forward
+ */
+static inline int collapse_submissions(struct atlas_submission *a,
+		struct atlas_submission *b, int adapt_sexec) {
+	
+	ktime_t start_a, start_b, end, move;
+	//can we move submission a forward? if not, we are ready
+	if (likely(ktime_equal(a->deadline, a->sdeadline)))
+		return 0;
+	
+	//adapt the deadline of the submission
+	start_a = get_submission_start(a);
+	start_b = get_submission_start(b);
+	end = ktime_min(a->deadline, start_b);
+
+	//end is either the start of the next submission or the real deadline
+	
+	//save the movement
+	move = ktime_sub(end, a->sdeadline);
+	a->sdeadline = end;
+	
+	//extend execution time?
+	if (!adapt_sexec || likely(ktime_equal(a->exectime, a->sexectime))) {
+		//no, but we moved the start, so there is a gap before
+		return 1;
+	}
+
+	//extend the execution time
+	a->sexectime = ktime_min(a->exectime, ktime_add(move, a->sexectime));
+	if (ktime_equal(start_a, get_submission_start(a))) {
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * close gaps, called when a submission is removed or when its exec time was updated
+ */
+static inline void close_gaps(struct atlas_submission *submission, int adapt_sexec) {
+	struct atlas_submission *prev;
+	while((prev = pick_prev_submission(submission))) {
+		if (!collapse_submissions(prev, submission, adapt_sexec))
+			break;
+		submission = prev;
+	}
+
+}
+
+/*
+ * calculate the gap between two submissions
+ */
+static inline ktime_t calc_gap(struct atlas_submission *a, struct atlas_submission *b) {
+	ktime_t start = get_submission_start(b);
+	ktime_t ret = ktime_sub(start, a->sdeadline);
+
+	BUG_ON(ktime_to_ns(ret) < 0);
+	return ret;
+}
+
+
+/*
+ * adapt sexectime of the submission
+ * return
+ * 		0 iff admission without shrinking execution time possible
+ * 		1 iff admission possible by shrinking execution time
+ * 		-1 no admission possible
+ */
+static inline int adapt_sexectime(struct atlas_rq *atlas_rq,
+		struct atlas_submission *submission, ktime_t now) {
+
+	struct atlas_submission *prev, *s;
+	ktime_t start, tmp, ret;
+
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	if (ktime_cmp(now, submission->deadline) >= 0) {
+		submission->sexectime = ktime_set(0,0);
+		return -1;
+	}
+
+	s = find_submission_pos(atlas_rq, submission);
+
+	ret = ktime_set(0,0);
+	//if there is no submission behind the new submission
+	if (!s) {
+		DEBUG(DEBUG_ADAPT_SEXEC, "no submission with later deadline");
+		//get the latest submission
+		s = pick_last_submission(atlas_rq);
+		if (!s) {
+			//no submission
+			BUG_ON(!RB_EMPTY_ROOT(&atlas_rq->submissions));
+			//time left for execution
+			ret = ktime_sub(submission->deadline, now);
+			DEBUG(DEBUG_ADAPT_SEXEC, "no submission with earlier deadline");
+			goto out;
+		}
+		//there is some time left behind the last submission
+		tmp = ktime_sub(submission->deadline, s->sdeadline);
+		BUG_ON(ktime_neg(tmp));//would mean, that deadline of s is before our deadline
+		ret = ktime_add(ret, tmp);
+		DEBUG(DEBUG_ADAPT_SEXEC, "submission with earlier deadline: gap=%lld", ktime_to_ms(ret));
+	} else {
+		start = get_submission_start(s);
+		//if start >= submission->deadline, we cannot use the full gap
+		if (ktime_cmp(submission->deadline, start) < 0) {
+			tmp = ktime_sub(submission->deadline, start);
+			ret = ktime_add(ret, tmp); //ret is negative now
+			DEBUG(DEBUG_ADAPT_SEXEC,
+				"submission with later deadline, sub from gap=%lld", ktime_to_ms(ret));
+		}
+	}
+
+	//sum up the gaps between the submissions;
+	while ((prev = pick_prev_submission(s))) {
+		if (ktime_cmp(now, prev->sdeadline) >= 0)
+			break;
+		ret = ktime_add(ret, calc_gap(prev, s));
+		s = prev;
+	}
+	DEBUG(DEBUG_ADAPT_SEXEC, "sum of gaps and end=%lld", ktime_to_ms(ret));
+	
+	start = get_submission_start(s);
+	//add time at the beginning
+	if (ktime_cmp(now, start) < 0) {
+		ret = ktime_add(ret, ktime_sub(start, now));
+		DEBUG(DEBUG_ADAPT_SEXEC, "additional start time, sum=%lld", ktime_to_ms(ret));
+	}
+
+out:
+	
+	if (ktime_cmp(ret, ktime_set(0,0)) <= 0) {
+		submission->sexectime = ktime_set(0,0);
+		DEBUG(DEBUG_ADAPT_SEXEC, "no addmission possible");
+		return -1;
+	}
+	
+	if (ktime_cmp(ret, submission->exectime) < 0) {
+		submission->sexectime = ret;
+		DEBUG(DEBUG_ADAPT_SEXEC, "admission with shrinked exectime");
+		return 1;
+	}
+	
+	DEBUG(DEBUG_ADAPT_SEXEC, "admission possible");
+	return 0;
+}
+
+/*
+ * must be called with rcu_read_lock()
+ */
+static void assign_rq_submission(struct atlas_submission *submission) {
+	struct rq *rq;
+	
+	struct rb_node **link;
+	struct rb_node *parent = NULL, *node, *snode;
+	struct atlas_submission *entry;
+	
+	get_submission(submission);
+	
+	rq = task_rq(pid_task(submission->pid, PIDTYPE_PID));
+	link = &rq->atlas.submissions.rb_node;
+	
+	assert_raw_spin_locked(&rq->atlas.lock);
+	
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct atlas_submission, rb_node);
+		
+		if (submission_before(submission, entry))
+			link = &parent->rb_left;
+		else
+			link = &parent->rb_right;
+	}
+	
+	rb_link_node(&submission->rb_node, parent, link);
+	rb_insert_color(&submission->rb_node, &rq->atlas.submissions);	
+	
+	/*
+	 * Neue Submission ist bzgl. der deadline jetzt korrekt eingeordnet.
+	 * Allerdings müssen noch die scheduled deadlines angepasst werden.
+	 * submission: iterator
+	 * entry: Nachfolger
+	 */
+	
+	snode = &submission->rb_node;
+	node = rb_next(snode);
+	
+	//Nachfolger?
+	if (node) {
+		entry = rb_entry(node, struct atlas_submission, rb_node);
+		if (collision(submission, entry))
+		{
+			resolve_collision(submission, entry);
+		}
+	}
+	
+	node = snode;
+	while ((node = rb_prev(node))) {
+		entry = rb_entry(node, struct atlas_submission, rb_node);
+		if (!collision(entry, submission))
+			break;
+		resolve_collision(entry, submission);
+		submission = entry;
+	}
+
+}
+
+static int update_execution_time(struct atlas_rq *atlas_rq,
+	struct atlas_submission *submission, ktime_t delta_exec) {
+	
+	int ret = 0;
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	submission->exectime = ktime_sub(submission->exectime, delta_exec); 
+
+	if (unlikely(ktime_neg(submission->exectime))) {
+		submission->exectime = ktime_set(0,0);
+		submission->sexectime = ktime_set(0,0);
+		ret = 1;
+	}
+
+	submission->sexectime = ktime_sub(submission->sexectime, delta_exec);
+	if (ktime_neg(submission->sexectime)) {
+		submission->sexectime = ktime_set(0,0);
+		ret = 2;
+	}
+
+	//adapt addmission plan
+	close_gaps(submission, 0);
+
+	return ret;
+}
+
+/*
+ * lock must be hold!
+ */
+static inline void erase_rq_submission(struct rq *rq,
+		struct atlas_submission *submission)
+{	
+	if (unlikely(!submission))
+		return;
+		
+	assert_raw_spin_locked(&rq->atlas.lock);
+	
+	if (likely(submission_in_rq(submission))) {
+		submission->sexectime = ktime_set(0,0);
+		close_gaps(submission, 1);
+		rb_erase(&submission->rb_node, &rq->atlas.submissions);
+		put_submission(submission);
+		RB_CLEAR_NODE(&submission->rb_node);
+	}	
+}
+
+/*
+ * determine if there is slack time left. submission tree has to be locked
+ * 
+ * return: 1 if there is slack time
+ * 		   0 if there is no slack time
+ * 
+ * the amount of slack time left is returned in ktime
+ */
+static int get_slacktime(struct atlas_rq *atlas_rq, ktime_t *slack) {
+	struct sched_atlas_entity *se;
+	struct atlas_submission *submission;
+	ktime_t start, sum, now;
+	
+	assert_raw_spin_locked(&atlas_rq->lock);
+	
+	se = pick_first_entity(atlas_rq);
+	BUG_ON(!se->submission);
+	
+	submission = se->submission;
+	//get start point
+	start = ktime_sub(submission->sdeadline, submission->sexectime);
+	
+	//sum up the execution time of the submissions before
+	sum = ktime_set(0,0);
+	while((submission = pick_prev_submission(submission))) {
+		sum = ktime_add(sum, submission->sexectime);
+	}
+
+	now = atlas_rq->slack_timer.base->get_time();
+	*slack = ktime_sub(ktime_sub(start, now), sum);
+
+	if (ktime_to_ns(*slack) > sysctl_sched_atlas_min_slack)
+		return 1;
+	else
+		return 0;
+}
+
+static void cleanup_rq(struct atlas_rq *atlas_rq) {
+	struct atlas_submission *tmp, *s = pick_first_submission(atlas_rq);
+	ktime_t now = atlas_rq->exec_timer.base->get_time();
+
+	assert_raw_spin_locked(&atlas_rq->lock);
+	while (unlikely(submission_missed_deadline(s, now))) {
+		struct task_struct *p = task_of_submission(s);
+		printk("now: %lld, submission :%lld\n",
+				ktime_to_ns(now),
+				ktime_to_ns(s->sdeadline));
+
+		if (p) {
+			printk_sched("drop Submission from rq; sub=%p pid=%d scheduler=%d sub_task=%p\n",
+					s, p->pid, p->policy, p->atlas.submission);
+		} else {
+			printk_sched("drop Submission of nonexistent task from rq; sub=%p\n", s);
+		}
+
+		tmp = s;
+		s = pick_next_submission(s);
+		erase_rq_submission(rq_of(atlas_rq), tmp);
+	}
+}
+
+/* 
+ * free pending submissions of a killed task
+ * called from do_exit()
+ *
+ * there might also be the timer
+ */
+void exit_atlas(struct task_struct *p) {
+	struct atlas_submission *submission, *tmp;
+	struct rq *rq = task_rq(p);
+	unsigned long flags;
+
+	hrtimer_cancel(&p->atlas.timer);
+
+	BUG_ON(in_interrupt());
+	//was there a submission in progress?
+	if ((submission = p->atlas.submission)) {
+		raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+		erase_rq_submission(rq, submission);
+		raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+		put_submission(submission);
+	}
+	list_for_each_entry_safe(submission, tmp, &p->atlas.submissions, list) {
+		raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+		erase_rq_submission(rq, submission);
+		raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+		put_submission(submission);
+	}
+}
+
+
+
+
+/*
+ * All the scheduling class methods:
+ */
+const struct sched_class atlas_sched_class = {
+	.next               = &fair_sched_class,
+	.enqueue_task       = enqueue_task_atlas,
+	.dequeue_task       = dequeue_task_atlas,
+	.yield_task         = yield_task_atlas,
+	//.yield_to_task		= yield_to_task_atlas,
+
+	.check_preempt_curr = check_preempt_curr_atlas,
+
+	.pick_next_task     = pick_next_task_atlas,
+	.put_prev_task      = put_prev_task_atlas,
+
+/**we do not support SMP so far*/
+#ifdef CONFIG_SMP
+	.select_task_rq     = select_task_rq_atlas,
+
+	//.rq_online		= rq_online_atlas,
+	//.rq_offline		= rq_offline_atlas,
+
+	//.task_waking		= task_waking_atlas,
+#endif
+
+	.set_curr_task      = set_curr_task_atlas,
+	.task_tick          = task_tick_atlas,
+	//.task_fork        = task_fork_atlas,
+
+	.prio_changed       = prio_changed_atlas,
+	.switched_from      = switched_from_atlas,
+	.switched_to        = switched_to_atlas,
+
+	.get_rr_interval    = get_rr_interval_atlas,
+
+};
+
+
+
+const struct sched_class atlas_fallback_sched_class = {
+	.next               = &idle_sched_class,
+	.pick_next_task     = pick_next_task_atlas_fallback,
+};
+
+
+SYSCALL_DEFINE0(atlas_debug)
+{
+#ifdef ATLAS_DEBUG
+	struct task_struct *g, *p;
+	struct rq *rq = &per_cpu(runqueues, 3);
+	int i;	
+	unsigned long flags;
+
+	printk(KERN_INFO "SYS_DEBUG: ##############################\n");
+	for_each_online_cpu(i) {
+		rq = &per_cpu(runqueues,i);
+		//debug_rq(&per_cpu(runqueues,i));
+		raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+		debug_submissions(&rq->atlas);
+		raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+	}
+	do_each_thread(g, p) {
+		if (!list_empty(&p->atlas.submissions))
+			debug_task(p);
+	} while_each_thread(g, p);
+	//struct atlas_submission *s;
+	raw_spin_lock_irqsave(&rq->atlas.lock, flags);
+	//s = pick_last_submission(&rq->atlas);
+	//printk_sched("submission: %p", s);
+	//update_execution_time(&rq->atlas, s, ktime_set(10,0));
+	//erase_rq_submission(rq, s);
+	//DEBUG_ON(DEBUG_SUBMISSIONS) {
+	
+	raw_spin_unlock_irqrestore(&rq->atlas.lock, flags);
+#endif
+	return 0;
+}
+
+extern int sched_setscheduler_no_check(struct task_struct *p, int policy,
+		       const struct sched_param *param);
+/*
+ * sys_atlas_next is somehow special:
+ * 
+ * There are cases within this functions where the task is running without
+ * having a submission!
+ * This isn't cool because we have to check in all scheduling functions
+ * whether there is a submission or not.
+ * 
+ */
+SYSCALL_DEFINE0(atlas_next)
+{
+	int ret = 0, missed = 0;
+	struct atlas_submission *old_sub;
+	struct sched_atlas_entity *se = &current->atlas;
+	struct rq *rq;
+	struct atlas_rq *atlas_rq;
+	unsigned long flags;
+	ktime_t now;
+
+	DEBUG(DEBUG_SYS_NEXT, "##START## pid=%d policy=%s submission=%p", current->pid,
+		current->policy == SCHED_NORMAL ? "CFS" :
+		current->policy == SCHED_ATLAS  ? "ATLAS" : "UNKNOWN",
+		se->submission);
+
+	/*
+	 * not necessary to check if the timer's callback run,
+	 * because we are already scheduled by CFS (scheduler
+	 * runs when returning from interrupt) in this case
+	 */
+	hrtimer_cancel(&se->timer);
+
+	se->flags &= ~ATLAS_EXHAUSTED_EXECTIME;
+	
+	preempt_disable();
+	
+	//we are done with the old submission
+	old_sub = se->submission;
+	se->submission = NULL;
+	put_submission(old_sub);
+	
+	
+	// we have no submission. if we are still scheduled by ATLAS,
+	// we enjoy a very high priority because of se->submission == NULL
+	
+	rq = task_rq(current);
+	atlas_rq = &rq->atlas;
+	
+	//update the admission plan -> there might be a gap now
+	if (current->policy == SCHED_ATLAS) {
+		raw_spin_lock_irqsave(&rq->lock, flags);
+		update_curr_atlas(rq);	
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	}
+	
+	//remove the old submission from the rq
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	erase_rq_submission(rq, old_sub);
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+	
+	preempt_enable();
+	
+	/* 
+	 * get new submission
+	 */
+	se->submission = pop_task_submission(se);
+
+	//deadline of the next submission already missed?
+	if (se->submission) {
+		now = se->timer.base->get_time();
+		if (ktime_neg(ktime_sub(se->submission->deadline, now)))
+			missed = 1;
+	}
+
+	BUG_ON(missed && current->policy == SCHED_ATLAS);
+
+	if (missed && current->policy == SCHED_NORMAL) {
+		DEBUG(DEBUG_SYS_NEXT, "already missed next deadline, don't switch to ATLAS");
+	}
+
+	
+	//switch (back) to SCHED_ATLAS if currently scheduled by cfs and
+	//not missed the deadline of the next submission
+	if (!missed && current->policy == SCHED_NORMAL) {
+		struct sched_param lparam = {0};
+		//reset MISSED_DEADLINE
+		se->flags &= ~ATLAS_MISSED_DEADLINE;
+		ret = sched_setscheduler(current, SCHED_ATLAS, &lparam);
+		WARN_ON(ret);
+		DEBUG(DEBUG_SYS_NEXT, "pid=%d switched back to ATLAS", current->pid);
+	}
+	
+	/* 
+	 * the following code is identical to wait queue implementation
+	 * when waiting for a specific event.
+	 * we are aware of the "lost wakup" problem
+	 */
+	if (se->submission) {
+		se->state = ATLAS_RUNNING;
+		/*
+		 * important if the previous submission has not missed deadline
+		 * make sure that this is the right task to run
+		 */
+		//check_preempt_from_syscall();
+		
+		//reschedule task
+		set_tsk_need_resched(current);
+		goto out;
+	}
+	
+	se->state = ATLAS_BLOCKED;
+	
+	for(;;) {
+		struct sched_param lparam = {0};
+		
+		set_current_state(TASK_INTERRUPTIBLE);
+		/*
+		 * Checking once again if there is a submission makes lost update
+		 * impossible.
+		 */
+		if ((se->submission = pop_task_submission(se)))
+			break;
+		DEBUG(DEBUG_SYS_NEXT, "pid=%d no submission, call schedule now", current->pid);
+
+		if (likely(!signal_pending(current))) {
+			schedule();
+			continue;
+		}
+		
+		ret = sched_setscheduler_nocheck(current, SCHED_NORMAL, &lparam);
+		WARN_ON(ret);
+		ret = -EINTR;
+		goto out_no_timer;
+	}
+
+	__set_current_state(TASK_RUNNING);
+	se->state = ATLAS_RUNNING;
+
+	DEBUG(DEBUG_SYS_NEXT, "pid=%d submission=%p submission->d=%llu",
+		current->pid, se->submission, ktime_to_us(se->submission->deadline));
+	
+out:
+	/*
+	 * setup new timer
+	 * if the deadline has already passed, the callback will be called
+	 * resulting in a scheduler switch to CFS
+	 */
+	DEBUG(DEBUG_SYS_NEXT, "pid=%d setup timer for submission %p (need_resched=%d).",
+		current->pid, se->submission, test_tsk_need_resched(current));
+	hrtimer_start(&se->timer, se->submission->deadline, HRTIMER_MODE_ABS_PINNED);
+out_no_timer:	
+	return ret;
+
+}
+
+
+SYSCALL_DEFINE3(atlas_submit, pid_t, pid, struct timeval __user *,
+					exectime, struct timeval __user *, deadline)
+					
+{
+	struct timeval lexectime;
+	struct timeval ldeadline;
+	struct atlas_submission *submission;
+	struct task_struct *t;
+	int ret = 0;
+	ktime_t now;
+	struct atlas_rq *atlas_rq;
+	unsigned long flags;
+
+	preempt_disable();
+	DEBUG(DEBUG_SYS_SUBMIT, "pid=%u, exectime=0x%p, deadline=0x%p",
+		pid, exectime, deadline);
+			
+	if (!exectime || !deadline || pid < 0)
+		return -EINVAL;
+					
+	if (copy_from_user(&lexectime, exectime, sizeof(struct timeval)) ||
+		copy_from_user(&ldeadline, deadline, sizeof(struct timeval))) {
+		DEBUG(DEBUG_SYS_SUBMIT, "bad address");
+		return -EFAULT;
+	}
+	DEBUG(DEBUG_SYS_SUBMIT, "pid=%u, exectime=%lld, deadline=%lld",
+		pid,
+		ktime_to_ms(timeval_to_ktime(lexectime)),
+		ktime_to_ms(timeval_to_ktime(ldeadline)));
+	
+	//allocate memory for a new submission
+	//we are not allowed to block when holding rcu_read_lock
+	submission = kmalloc(sizeof(struct atlas_submission), 0);
+	DEBUG(DEBUG_SYS_SUBMIT, "submission=%p", submission);
+	if (submission == NULL) {
+		return -ENOMEM;
+	}
+
+	rcu_read_lock();
+	//search for pid and reference it
+	submission->pid = find_get_pid(pid);
+	
+	if (!submission->pid) {
+		//free submission
+		kfree(submission);
+		ret = -ESRCH;
+		goto out;
+	}
+	
+	//we have the submission
+	atomic_set(&submission->count, 1);
+	
+	t = pid_task(submission->pid, PIDTYPE_PID);
+	BUG_ON(!t);
+	
+	submission->deadline = ktime_add(t->atlas.timer.base->get_time(),
+						timeval_to_ktime(ldeadline));
+	//submission->deadline = timeval_to_ktime(ldeadline);
+	submission->exectime = timeval_to_ktime(lexectime);
+	
+	submission->sdeadline = submission->deadline;
+	submission->sexectime = submission->exectime;
+
+	atlas_rq = &task_rq(t)->atlas;
+	raw_spin_lock_irqsave(&atlas_rq->lock, flags);
+	now = atlas_rq->exec_timer.base->get_time();
+	ret = adapt_sexectime(&task_rq(t)->atlas, submission, now);
+
+	switch(ret) {
+	case 0: DEBUG(DEBUG_SYS_SUBMIT, "sexectime == exectime"); break;
+	case 1: DEBUG(DEBUG_SYS_SUBMIT, "sexectime < exectime"); break;
+	case -1: DEBUG(DEBUG_SYS_SUBMIT, "sexectime == 0"); break;
+	}
+
+	assign_rq_submission(submission);
+
+	/*
+	DEBUG_ON(DEBUG_SUBMISSIONS) {
+		debug_submissions(atlas_rq);
+	}*/
+
+	raw_spin_unlock_irqrestore(&atlas_rq->lock, flags);
+
+	assign_task_submission(submission);
+	
+	put_submission(submission);
+	
+out:
+	DEBUG(DEBUG_SYS_SUBMIT, "ready");
+	rcu_read_unlock();
+	preempt_enable();
+	return ret;
+}
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 39c44fa..860b667 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1061,12 +1061,17 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 	if (p->sched_class == rq->curr->sched_class) {
 		rq->curr->sched_class->check_preempt_curr(rq, p, flags);
 	} else {
-		for_each_class(class) {
-			if (class == rq->curr->sched_class)
-				break;
-			if (class == p->sched_class) {
-				resched_task(rq->curr);
-				break;
+		if (unlikely(rq->curr->sched_class == &atlas_sched_class) &&
+			unlikely(rq->curr->atlas.flags & ATLAS_FALLBACK))
+			resched_task(rq->curr);
+		else {
+			for_each_class(class) {
+				if (class == rq->curr->sched_class)
+					break;
+				if (class == p->sched_class) {
+					resched_task(rq->curr);
+					break;
+				}
 			}
 		}
 	}
@@ -1695,6 +1700,8 @@ int wake_up_state(struct task_struct *p, unsigned int state)
 	return try_to_wake_up(p, state, 0);
 }
 
+extern enum hrtimer_restart atlas_timer_task_function(struct hrtimer *timer);
+
 /*
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
@@ -1718,6 +1725,12 @@ static void __sched_fork(struct task_struct *p)
 #endif
 
 	INIT_LIST_HEAD(&p->rt.run_list);
+	
+	/*ATLAS stuff*/
+	INIT_LIST_HEAD(&p->atlas.submissions);
+	p->atlas.on_rq = 0;
+	hrtimer_init(&p->atlas.timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	p->atlas.timer.function = &atlas_timer_task_function;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
@@ -3353,16 +3366,20 @@ pick_next_task(struct rq *rq)
 	if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
 		p = fair_sched_class.pick_next_task(rq);
 		if (likely(p))
-			return p;
+			goto out;
 	}
 
 	for_each_class(class) {
 		p = class->pick_next_task(rq);
 		if (p)
-			return p;
+			goto out;
 	}
 
-	BUG(); /* the idle class will always have a runnable task */
+out:
+	BUG_ON(!p); /* the idle class will always have a runnable task */
+	
+	trace_sched_pick_next_task(p);
+	return p;
 }
 
 /*
@@ -3373,7 +3390,8 @@ static void __sched __schedule(void)
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
 	struct rq *rq;
-	int cpu;
+	int cpu, ret;
+	struct sched_param lparam = {0};
 
 need_resched:
 	preempt_disable();
@@ -3381,12 +3399,33 @@ need_resched:
 	rq = cpu_rq(cpu);
 	rcu_note_context_switch(cpu);
 	prev = rq->curr;
+	
+	/*
+	 * SCHED_ATLAS: if the prev has ATLAS_MISSED_DEADLINE flag, move it to CFS
+	 */
+	if (unlikely(prev->policy == SCHED_ATLAS) &&
+			unlikely(prev->atlas.flags & (ATLAS_MISSED_DEADLINE | ATLAS_EXHAUSTED_EXECTIME | ATLAS_OVERLOAD))) {
+		if (prev->atlas.flags & ATLAS_MISSED_DEADLINE)
+			printk_sched("schedule(): ATLAS_MISSED_DEADLINE\n");
+		if (prev->atlas.flags & ATLAS_EXHAUSTED_EXECTIME)
+			printk_sched("schedule(): ATLAS_EXHAUSTED_EXECTIME\n");
+		if (prev->atlas.flags & ATLAS_OVERLOAD)
+			printk_sched("schedule(): ATLAS_OVERLOAD\n");
+		ret = sched_setscheduler_nocheck(prev, SCHED_NORMAL, &lparam);
+		WARN_ON(ret);
+		//prev->atlas.flags &= ~ATLAS_MISSED_DEADLINE; //done in switched from
+		//clear_tsk_need_resched(prev); mhh? reschedule the task, because ATLAS has likely a more important one
+		sched_preempt_enable_no_resched();
+		printk_sched("schedule(): switch scheduler to cfs of pid=%d\n", prev->pid);
+		//return;
+		goto need_resched;
+	}
 
 	schedule_debug(prev);
 
 	if (sched_feat(HRTICK))
 		hrtick_clear(rq);
-
+	
 	raw_spin_lock_irq(&rq->lock);
 
 	switch_count = &prev->nivcsw;
@@ -4223,6 +4262,8 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 	p->prio = rt_mutex_getprio(p);
 	if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	else if (policy == SCHED_ATLAS)
+		p->sched_class = &atlas_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 	set_load_weight(p);
@@ -4266,7 +4307,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy != SCHED_ATLAS)
 			return -EINVAL;
 	}
 
@@ -7267,6 +7308,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		init_atlas_rq(&rq->atlas);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c099cc6..38b5249d 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -615,6 +615,10 @@ static u64 __sched_period(unsigned long nr_running)
 	return period;
 }
 
+u64 cfs_sched_period(unsigned long nr_running) {
+	return __sched_period(nr_running);
+}
+
 /*
  * We calculate the wall-time slice from the period by taking a part
  * proportional to the weight.
@@ -5259,7 +5263,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &atlas_fallback_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 573e1ca..b272fa6 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2038,7 +2038,7 @@ static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)
 }
 
 const struct sched_class rt_sched_class = {
-	.next			= &fair_sched_class,
+	.next			= &atlas_sched_class,
 	.enqueue_task		= enqueue_task_rt,
 	.dequeue_task		= dequeue_task_rt,
 	.yield_task		= yield_task_rt,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5cf093f..5d4aaa3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -309,6 +309,40 @@ struct rt_rq {
 #endif
 };
 
+#define ATLAS_MISSED_DEADLINE    0x01
+#define ATLAS_OVERLOAD           0x02
+#define ATLAS_SYS_NEXT 		     0x04
+#define ATLAS_EXHAUSTED_EXECTIME 0x08
+#define ATLAS_BAD_TASK           0x10
+#define ATLAS_FALLBACK           0x20
+#define ATLAS_SLACK              0x40
+
+//needs to be defined here because of trace stuff
+struct atlas_submission {
+	struct list_head list;
+	struct rb_node rb_node;
+	struct pid *pid;  //used to map submission -> map AND to distinguish task and gap
+	ktime_t exectime; //relative
+	ktime_t deadline; //absolut
+	ktime_t sdeadline;
+	ktime_t sexectime;
+	atomic_t count;
+};
+
+struct atlas_rq {
+	struct sched_atlas_entity *curr, *timer_se;
+	struct rb_root     tasks_timeline;
+	struct rb_node *rb_leftmost_se;
+	struct rb_root     submissions;
+	raw_spinlock_t			lock;
+	struct atlas_submission *yielded_submission;
+	int nr_runnable;
+	struct hrtimer exec_timer;
+	struct hrtimer slack_timer;
+	ktime_t slack_end;
+	unsigned long flags;
+};
+
 #ifdef CONFIG_SMP
 
 /*
@@ -370,6 +404,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct atlas_rq atlas;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -847,7 +882,9 @@ enum cpuacct_stat_index {
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
+extern const struct sched_class atlas_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class atlas_fallback_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1143,6 +1180,7 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_atlas_rq(struct atlas_rq *atlas_rq);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 4ab1187..037c830 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -339,6 +339,20 @@ static struct ctl_table kern_table[] = {
 	},
 #endif
 	{
+		.procname	= "sched_atlas_min_slack",
+		.data		= &sysctl_sched_atlas_min_slack,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_atlas_slice",
+		.data		= &sysctl_sched_atlas_slice,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
 		.procname	= "sched_rt_period_us",
 		.data		= &sysctl_sched_rt_period,
 		.maxlen		= sizeof(unsigned int),
