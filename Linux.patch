diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0657368..8c85cb8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,8 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+/* SCHED_CSS: completely stupid scheduler */
+#define SCHED_CSS       6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1225,6 +1227,10 @@ struct sched_rt_entity {
 #endif
 };
 
+struct sched_css_entity {
+    struct list_head run_list;
+};
+
 struct rcu_node;
 
 enum perf_event_task_context {
@@ -1252,6 +1258,7 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+    struct sched_css_entity css;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 9a7dd35..e351508 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o
+obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o css.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b342f57..136a782 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1737,7 +1737,7 @@ void sched_fork(struct task_struct *p)
 	}
 
 	if (!rt_prio(p->prio))
-		p->sched_class = &fair_sched_class;
+		p->sched_class = &css_sched_class;
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -3972,6 +3972,8 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 	p->prio = rt_mutex_getprio(p);
 	if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+    else if (policy == SCHED_CSS)
+        p->sched_class = &css_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 	set_load_weight(p);
@@ -4018,7 +4020,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy != SCHED_CSS)
 			return -EINVAL;
 	}
 
@@ -6881,6 +6883,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+        init_css_rq(&rq->css);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
diff --git a/kernel/sched/css.c b/kernel/sched/css.c
new file mode 100644
index 0000000..292f660
--- /dev/null
+++ b/kernel/sched/css.c
@@ -0,0 +1,230 @@
+#include "sched.h"
+
+//#define DEBUG_CSS
+
+const struct sched_class css_sched_class;
+
+void init_css_rq(struct css_rq *css_rq)
+{
+    //init runqueue
+    css_rq->nr_running = 0;
+    css_rq->curr = NULL;
+    css_rq->ticks = 0;
+    INIT_LIST_HEAD(&css_rq->queue);
+}
+
+static void update_curr_css(struct rq *rq)
+{
+    //copied from rt
+	struct task_struct *curr = rq->curr;
+	u64 delta_exec;
+
+	if (curr->sched_class != &css_sched_class)
+		return;
+
+	delta_exec = rq->clock_task - curr->se.exec_start;
+	if (unlikely((s64)delta_exec < 0))
+		delta_exec = 0;
+
+	schedstat_set(curr->se.statistics.exec_max, max(curr->se.statistics.exec_max, delta_exec));
+
+	curr->se.sum_exec_runtime += delta_exec;
+	account_group_exec_runtime(curr, delta_exec);
+
+	curr->se.exec_start = rq->clock_task;
+	cpuacct_charge(curr, delta_exec);
+
+}
+
+
+static void __debug_task_css(struct css_rq *css_rq)
+{
+#ifdef DEBUG_CSS
+    struct sched_css_entity *se;
+    struct task_struct *p;
+    printk(KERN_INFO "SCHED_CSS: ----- DEBUG task_list\n");
+    list_for_each_entry(se, &css_rq->queue, run_list) {
+        p = container_of(se, struct task_struct, css);
+        printk(KERN_INFO "SCHED_CSS: list_element: %p\n", p);
+    }
+#endif
+}
+
+static void __enqueue_task_css(struct css_rq *css_rq, struct task_struct *p)
+{
+    list_add_tail(&p->css.run_list, &css_rq->queue);
+    css_rq->nr_running++;
+}
+
+static void __dequeue_task_css(struct css_rq *css_rq, struct task_struct *p)
+{
+    list_del(&p->css.run_list);
+    css_rq->curr = NULL;
+    css_rq->nr_running--;
+}
+
+static void enqueue_task_css(struct rq *rq, struct task_struct *p, int flags)
+{
+#ifdef DEBUG_CSS
+    printk(KERN_INFO "SCHED_CSS: enqueue task (p=%p, rq=%p)\n", p, rq->curr);
+#endif
+    __debug_task_css(&rq->css);
+    __enqueue_task_css(&rq->css, p);
+    __debug_task_css(&rq->css);
+    inc_nr_running(rq);
+	return;
+}
+
+static void dequeue_task_css(struct rq *rq, struct task_struct *p, int flags)
+{
+#ifdef DEBUG_CSS
+    printk(KERN_INFO "SCHED_CSS: dequeue task (p=%p, rq=%p)\n", p, rq->curr);
+#endif
+    __debug_task_css(&rq->css);
+    __dequeue_task_css(&rq->css, p);
+    __debug_task_css(&rq->css);
+    dec_nr_running(rq);
+	return;
+}
+
+static void yield_task_css(struct rq *rq)
+{
+    __dequeue_task_css(&rq->css, rq->curr);
+    __enqueue_task_css(&rq->css, rq->curr);
+    return;
+}
+
+static void check_preempt_curr_css(struct rq *rq, struct task_struct *p, int flags)
+{
+#ifdef DEBUG_CSS
+    printk(KERN_INFO "SCHED_CSS: check_preempt_curr\n");
+#endif
+	return;
+}
+
+static struct task_struct *pick_next_task_css(struct rq *rq)
+{
+    struct css_rq *css_rq = &rq->css;
+    struct sched_css_entity *se;
+    struct list_head *next;
+    
+    if (css_rq->curr) {
+        css_rq->curr->se.exec_start = rq->clock_task;
+        return css_rq->curr;
+    }
+    
+    if (css_rq->nr_running == 0)
+        return NULL;
+    
+    //find next task; we know that there is at least one element in the list
+    if (list_empty(&css_rq->queue))
+        BUG();
+    
+    next = css_rq->queue.next;
+    se = list_entry(next, struct sched_css_entity, run_list);
+    css_rq->curr = container_of(se, struct task_struct, css);
+    
+    //rotate the list
+    list_rotate_left(&css_rq->queue);
+    css_rq->curr->se.exec_start = rq->clock_task;
+    return css_rq->curr;
+}
+
+static void put_prev_task_css(struct rq *rq, struct task_struct *p)
+{
+    //printk(KERN_INFO "SCHED_CSS: put_prev_task (p=%p, rq->curr=%p, p->on_rq=%d)\n", p, rq->curr, p->on_rq);
+    update_curr_css(rq);
+    //ignore
+	return;
+}
+
+static void set_curr_task_css(struct rq *rq)
+{
+    struct task_struct *p = rq->curr;
+#ifdef DEBUG_CSS
+    printk(KERN_INFO "SCHED_CSS: set_curr_task\n");
+#endif
+    rq->css.curr = p;
+    p->se.exec_start = rq->clock_task;
+    return;
+}
+
+static void task_tick_css(struct rq *rq, struct task_struct *p, int queued)
+{
+    //scheint auch zu funzen :)
+    //printk(KERN_INFO "SCHED_CSS: task_tick\n");
+    unsigned *ticks = &rq->css.ticks;
+    
+    update_curr_css(rq);
+    (*ticks)++;
+    if (*ticks >= 10) {
+        *ticks = 0;
+        rq->css.curr = NULL;
+        resched_task(p);
+    }
+    return;
+}
+
+static void prio_changed_css(struct rq *rq, struct task_struct *p, int oldprio)
+{
+    printk(KERN_INFO "SCHED_CSS: prio_changed\n");
+    return;
+}
+
+static void switched_from_css(struct rq *rq, struct task_struct *p)
+{
+    printk(KERN_INFO "SCHED_CSS: switched_from\n");
+    return;
+}
+
+static void switched_to_css(struct rq *rq, struct task_struct *p)
+{
+    printk(KERN_INFO "SCHED_CSS: switched_to\n");
+    return;
+}   
+
+static unsigned int get_rr_interval_css(struct rq *rq, struct task_struct *task)
+{
+    printk(KERN_INFO "SCHED_CSS: get_rr_interval\n");
+    return 0;
+}
+
+
+
+
+/*
+ * All the scheduling class methods:
+ */
+const struct sched_class css_sched_class = {
+	.next			= &idle_sched_class,
+	.enqueue_task		= enqueue_task_css,
+	.dequeue_task		= dequeue_task_css,
+	.yield_task		= yield_task_css,
+	//.yield_to_task		= yield_to_task_css,
+
+	.check_preempt_curr	= check_preempt_curr_css,
+
+	.pick_next_task		= pick_next_task_css,
+	.put_prev_task		= put_prev_task_css,
+
+/**we do not support SMP so far*/
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_css,
+
+	.rq_online		= rq_online_css,
+	.rq_offline		= rq_offline_css,
+
+	.task_waking		= task_waking_css,
+#endif
+
+	.set_curr_task          = set_curr_task_css,
+	.task_tick		= task_tick_css,
+	//.task_fork		= task_fork_css,
+
+	.prio_changed		= prio_changed_css,
+	.switched_from		= switched_from_css,
+	.switched_to		= switched_to_css,
+
+	.get_rr_interval	= get_rr_interval_css,
+
+};
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index aca16b8..4dfe277 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5561,7 +5561,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &css_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 98c0c26..7fd48b1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -283,6 +283,12 @@ struct cfs_rq {
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
 
+struct css_rq {
+    struct list_head queue;
+    unsigned nr_running, ticks;
+    struct task_struct *curr;
+};
+
 static inline int rt_bandwidth_enabled(void)
 {
 	return sysctl_sched_rt_runtime >= 0;
@@ -382,6 +388,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+    struct css_rq css;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -854,6 +861,7 @@ enum cpuacct_stat_index {
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class css_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1151,6 +1159,7 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_css_rq(struct css_rq *css_rq);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
